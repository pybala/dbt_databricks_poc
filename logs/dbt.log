[0m21:22:56.812830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca9eb72b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca7e9b550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca7e9b520>]}


============================== 21:22:56.815195 | d7e5c633-cb0f-45a8-9fc8-9a16df99aef2 ==============================
[0m21:22:56.815195 [info ] [MainThread]: Running with dbt=1.7.1
[0m21:22:56.816053 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m21:22:56.816701 [info ] [MainThread]: dbt version: 1.7.1
[0m21:22:56.817099 [info ] [MainThread]: python version: 3.10.12
[0m21:22:56.817464 [info ] [MainThread]: python path: /home/bala/python_venvs/dbt-datbricks-env/bin/python
[0m21:22:56.817825 [info ] [MainThread]: os info: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m21:22:57.727553 [info ] [MainThread]: Using profiles dir at /home/bala/apps/pocs/dbt/databricks_poc
[0m21:22:57.728064 [info ] [MainThread]: Using profiles.yml file at /home/bala/apps/pocs/dbt/databricks_poc/profiles.yml
[0m21:22:57.728628 [info ] [MainThread]: Using dbt_project.yml file at /home/bala/apps/pocs/dbt/databricks_poc/dbt_project.yml
[0m21:22:57.729088 [info ] [MainThread]: adapter type: databricks
[0m21:22:57.729501 [info ] [MainThread]: adapter version: 1.7.0
[0m21:22:57.804855 [info ] [MainThread]: Configuration:
[0m21:22:57.805542 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m21:22:57.806249 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m21:22:57.806923 [info ] [MainThread]: Required dependencies:
[0m21:22:57.807641 [debug] [MainThread]: Executing "git --help"
[0m21:22:57.818426 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:22:57.819019 [debug] [MainThread]: STDERR: "b''"
[0m21:22:57.819374 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m21:22:57.819790 [info ] [MainThread]: Connection:
[0m21:22:57.820285 [info ] [MainThread]:   host: adb-4865047326705826.6.azuredatabricks.net
[0m21:22:57.820684 [info ] [MainThread]:   http_path: sql/protocolv1/o/4865047326705826/1110-180114-rnxfoute
[0m21:22:57.821085 [info ] [MainThread]:   catalog: hive_metastore
[0m21:22:57.821470 [info ] [MainThread]:   schema: default
[0m21:22:57.821948 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m21:22:57.822553 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m21:22:57.822913 [debug] [MainThread]: Using databricks connection "debug"
[0m21:22:57.823284 [debug] [MainThread]: On debug: select 1 as id
[0m21:22:57.823621 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:27:54.165548 [debug] [MainThread]: SQL status: OK in 296.3399963378906 seconds
[0m21:27:54.629412 [debug] [MainThread]: On debug: Close
[0m21:27:55.251223 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m21:27:55.253588 [info ] [MainThread]: [32mAll checks passed![0m
[0m21:27:55.258932 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 298.488, "process_user_time": 3.51327, "process_kernel_time": 0.828413, "process_mem_max_rss": "206892", "process_in_blocks": "2176", "process_out_blocks": "32"}
[0m21:27:55.262708 [debug] [MainThread]: Command `dbt debug` succeeded at 21:27:55.261799 after 298.49 seconds
[0m21:27:55.264854 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m21:27:55.267272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca9eb72b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca7e9b460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca7e0ba90>]}
[0m21:27:55.269546 [debug] [MainThread]: Flushing usage events
[0m12:12:17.631748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b50ddf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b30bf610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b30bf5b0>]}


============================== 12:12:17.634418 | 4f61cf0f-5e40-464a-a721-4f740757f8d5 ==============================
[0m12:12:17.634418 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:12:17.634943 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:12:18.937948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b30bf700>]}
[0m12:12:19.074988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668c6c7d00>]}
[0m12:12:19.075573 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:12:19.088164 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:12:19.088762 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:12:19.089170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b302f640>]}
[0m12:12:20.682133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66868a5600>]}
[0m12:12:20.700434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668c838be0>]}
[0m12:12:20.700939 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:12:20.701416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668c6c6c50>]}
[0m12:12:20.703069 [info ] [MainThread]: 
[0m12:12:20.703923 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:12:20.705102 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:12:20.705595 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:12:20.705947 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:12:20.706272 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:12:22.969399 [debug] [ThreadPool]: SQL status: OK in 2.259999990463257 seconds
[0m12:12:22.982469 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:12:23.507159 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m12:12:23.507940 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m12:12:23.517254 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:23.517674 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m12:12:23.518034 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m12:12:23.518368 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:12:25.791412 [debug] [ThreadPool]: SQL status: OK in 2.2699999809265137 seconds
[0m12:12:25.793078 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:12:25.793498 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m12:12:25.793848 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:12:25.794174 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m12:12:26.130349 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_default)
[0m12:12:26.135399 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:12:26.135810 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:12:26.136186 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:12:27.387415 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m12:12:27.391321 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:12:27.664885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668664b340>]}
[0m12:12:27.665468 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:27.665829 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:12:27.666522 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:12:27.667109 [info ] [MainThread]: 
[0m12:12:27.678445 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:12:27.679090 [info ] [Thread-1 (]: 1 of 2 START seed file default.product ......................................... [RUN]
[0m12:12:27.680405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:12:27.680980 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:12:27.681623 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:12:27.681356 => 12:12:27.681359
[0m12:12:27.682238 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:12:27.733332 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:27.733756 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:12:27.734281 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` 2023-11-17 10:00:00` timestamp,` 2023-11-17 10:00:00_2` timestamp)
    
    using delta
    
    
    
    
    
  
[0m12:12:27.734693 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:12:32.112024 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` 2023-11-17 10:00:00` timestamp,` 2023-11-17 10:00:00_2` timestamp)
    
    using delta
    
    
    
    
    
  
[0m12:12:32.112534 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:32.113586 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Apples` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:12:32.114623 [debug] [Thread-1 (]: Databricks adapter: operation-id: d63bb795-0935-4ac7-a418-fd4ee3ca7b29
[0m12:12:32.115208 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:12:27.682480 => 12:12:32.114957
[0m12:12:32.115615 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:12:32.116000 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:12:32.116390 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:12:32.407177 [debug] [Thread-1 (]: Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:32.407832 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6686666200>]}
[0m12:12:32.408386 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file default.product ................................. [[31mERROR[0m in 4.73s]
[0m12:12:32.409263 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:12:32.410016 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:12:32.410682 [info ] [Thread-1 (]: 2 of 2 START seed file default.product_category ................................ [RUN]
[0m12:12:32.411526 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:12:32.411952 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:12:32.412453 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:12:32.412199 => 12:12:32.412201
[0m12:12:32.412979 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:12:32.418879 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:32.419228 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:12:32.419716 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:12:32.420128 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:12:34.159213 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:12:34.159755 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:34.161070 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Fruits` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:12:34.162091 [debug] [Thread-1 (]: Databricks adapter: operation-id: 190f99c0-7f7d-42aa-a4de-e6c521724690
[0m12:12:34.162710 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:12:32.413224 => 12:12:34.162427
[0m12:12:34.163221 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:12:34.163653 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:12:34.164054 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:12:34.435111 [debug] [Thread-1 (]: Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:34.435674 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f61cf0f-5e40-464a-a721-4f740757f8d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668668b940>]}
[0m12:12:34.436245 [error] [Thread-1 (]: 2 of 2 ERROR loading seed file default.product_category ........................ [[31mERROR[0m in 2.02s]
[0m12:12:34.437069 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:12:34.438909 [debug] [MainThread]: On master: ROLLBACK
[0m12:12:34.439260 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:12:35.201911 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:12:35.202376 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:12:35.202710 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:12:35.203071 [debug] [MainThread]: On master: ROLLBACK
[0m12:12:35.203401 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:12:35.203720 [debug] [MainThread]: On master: Close
[0m12:12:35.519636 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:12:35.520063 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:12:35.520686 [info ] [MainThread]: 
[0m12:12:35.521097 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 14.82 seconds (14.82s).
[0m12:12:35.522082 [debug] [MainThread]: Command end result
[0m12:12:35.533544 [info ] [MainThread]: 
[0m12:12:35.534032 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m12:12:35.534526 [info ] [MainThread]: 
[0m12:12:35.535106 [error] [MainThread]:   Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:35.535602 [info ] [MainThread]: 
[0m12:12:35.535965 [error] [MainThread]:   Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:12:35.536379 [info ] [MainThread]: 
[0m12:12:35.536765 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m12:12:35.538356 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 17.947575, "process_user_time": 5.25496, "process_kernel_time": 0.872483, "process_mem_max_rss": "226652", "process_in_blocks": "206992", "process_out_blocks": "3456", "command_success": false}
[0m12:12:35.538903 [debug] [MainThread]: Command `dbt seed` failed at 12:12:35.538767 after 17.95 seconds
[0m12:12:35.539315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b50ddf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f668c889990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66866e0fd0>]}
[0m12:12:35.539710 [debug] [MainThread]: Flushing usage events
[0m12:26:40.731013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaf1f41f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaeff175b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaeff17550>]}


============================== 12:26:40.733460 | 8380be58-f243-4ee7-a52a-6fa8ba32f5ad ==============================
[0m12:26:40.733460 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:26:40.734153 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m12:26:41.694740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaeff176a0>]}
[0m12:26:41.834969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faac5495540>]}
[0m12:26:41.835577 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:26:41.845702 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:26:41.856659 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:26:41.857211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faac5495150>]}
[0m12:26:43.448319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaf6d80d0>]}
[0m12:26:43.459102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faac52a1c90>]}
[0m12:26:43.459624 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:26:43.460028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faac53a2b90>]}
[0m12:26:43.461436 [info ] [MainThread]: 
[0m12:26:43.462256 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:26:43.463482 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:26:43.463990 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:26:43.464378 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:26:43.464706 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:26:44.568016 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m12:26:44.570617 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:26:44.845980 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m12:26:44.846769 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m12:26:44.855156 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:26:44.855600 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m12:26:44.856001 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m12:26:44.856431 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:26:46.889303 [debug] [ThreadPool]: SQL status: OK in 2.0299999713897705 seconds
[0m12:26:46.890366 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:26:46.890742 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m12:26:46.891130 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:26:46.891466 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m12:26:47.158534 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m12:26:47.163460 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m12:26:47.164105 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m12:26:47.164555 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:26:48.362603 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m12:26:48.366120 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m12:26:48.657863 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m12:26:48.660522 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:26:48.660917 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:26:48.661289 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:26:49.835052 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m12:26:49.837858 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:26:50.149965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaf58e500>]}
[0m12:26:50.150589 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:26:50.150945 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:26:50.151693 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:26:50.152125 [info ] [MainThread]: 
[0m12:26:50.154724 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:26:50.155309 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product .............................. [RUN]
[0m12:26:50.156179 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:26:50.156671 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:26:50.157272 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:26:50.156967 => 12:26:50.156970
[0m12:26:50.157778 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:26:50.208141 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:26:50.208766 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:26:50.209291 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default_poc_bronze`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` "2023-11-17 10:00:00"` string,` "2023-11-17 10:00:00"_2` string)
    
    using delta
    
    
    
    
    
  
[0m12:26:50.209706 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:26:51.772362 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default_poc_bronze`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` "2023-11-17 10:00:00"` string,` "2023-11-17 10:00:00"_2` string)
    
    using delta
    
    
    
    
    
  
[0m12:26:51.772921 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:51.773966 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Apples` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:26:51.774934 [debug] [Thread-1 (]: Databricks adapter: operation-id: b1509e79-a9ba-4bf9-a991-5cb1e1a2d9a4
[0m12:26:51.775496 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:26:50.158079 => 12:26:51.775263
[0m12:26:51.775915 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:26:51.776299 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:26:51.776672 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:26:52.114709 [debug] [Thread-1 (]: Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:52.115302 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaf605150>]}
[0m12:26:52.115852 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file default_poc_bronze.product ...................... [[31mERROR[0m in 1.96s]
[0m12:26:52.117167 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:26:52.117719 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:26:52.118536 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category ..................... [RUN]
[0m12:26:52.119463 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:26:52.119885 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:26:52.120402 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:26:52.120141 => 12:26:52.120144
[0m12:26:52.120784 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:26:52.127658 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:26:52.128088 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:26:52.128516 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:26:52.128902 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:26:53.696934 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:26:53.697473 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:53.698497 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Fruits` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:26:53.699506 [debug] [Thread-1 (]: Databricks adapter: operation-id: a3d01b5e-96ac-4965-9114-33be5bae239a
[0m12:26:53.700102 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:26:52.121030 => 12:26:53.699824
[0m12:26:53.700744 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:26:53.701179 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:26:53.701580 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:26:53.968264 [debug] [Thread-1 (]: Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:53.968878 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8380be58-f243-4ee7-a52a-6fa8ba32f5ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaf5b98a0>]}
[0m12:26:53.969426 [error] [Thread-1 (]: 2 of 2 ERROR loading seed file default_poc_bronze.product_category ............. [[31mERROR[0m in 1.85s]
[0m12:26:53.970291 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:26:53.972463 [debug] [MainThread]: On master: ROLLBACK
[0m12:26:53.973014 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:26:54.806700 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:26:54.807216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:26:54.807554 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:26:54.807886 [debug] [MainThread]: On master: ROLLBACK
[0m12:26:54.808233 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:26:54.808550 [debug] [MainThread]: On master: Close
[0m12:26:55.097067 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:26:55.097498 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:26:55.098148 [info ] [MainThread]: 
[0m12:26:55.098669 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 11.64 seconds (11.64s).
[0m12:26:55.099736 [debug] [MainThread]: Command end result
[0m12:26:55.108981 [info ] [MainThread]: 
[0m12:26:55.109608 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m12:26:55.110131 [info ] [MainThread]: 
[0m12:26:55.110580 [error] [MainThread]:   Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:55.111094 [info ] [MainThread]: 
[0m12:26:55.111655 [error] [MainThread]:   Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:26:55.112089 [info ] [MainThread]: 
[0m12:26:55.112598 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m12:26:55.113517 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 14.43189, "process_user_time": 5.364216, "process_kernel_time": 0.925906, "process_mem_max_rss": "223464", "process_out_blocks": "3456", "command_success": false, "process_in_blocks": "0"}
[0m12:26:55.114198 [debug] [MainThread]: Command `dbt seed` failed at 12:26:55.114066 after 14.43 seconds
[0m12:26:55.114701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaf1f41f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faac56a6410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaaf64a9b0>]}
[0m12:26:55.115212 [debug] [MainThread]: Flushing usage events
[0m12:45:43.083445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac4d189f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac4b15f5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac4b15f580>]}


============================== 12:45:43.086006 | dde2b049-8703-4694-a0b8-a5edaeb94ef8 ==============================
[0m12:45:43.086006 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:45:43.086668 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m12:45:44.080113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac4b15f6d0>]}
[0m12:45:44.219256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac246d1fc0>]}
[0m12:45:44.219866 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:45:44.230243 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:45:44.241116 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:45:44.241696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac246d2080>]}
[0m12:45:45.731763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac1e8d00d0>]}
[0m12:45:45.742057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac244e1810>]}
[0m12:45:45.742558 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:45:45.742992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac243e7eb0>]}
[0m12:45:45.744366 [info ] [MainThread]: 
[0m12:45:45.745437 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:45:45.746600 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:45:45.747109 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:45:45.747457 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:45:45.747817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:45:51.285643 [debug] [ThreadPool]: SQL status: OK in 5.539999961853027 seconds
[0m12:45:51.288356 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:45:51.769264 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m12:45:51.770078 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m12:45:51.778642 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:51.779015 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m12:45:51.779376 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m12:45:51.779730 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:45:57.130943 [debug] [ThreadPool]: SQL status: OK in 5.349999904632568 seconds
[0m12:45:57.132187 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:45:57.132642 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m12:45:57.133039 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:45:57.133402 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m12:45:57.414959 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_default)
[0m12:45:57.419674 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:45:57.420064 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:45:57.420392 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:45:58.788989 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m12:45:58.792658 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:45:59.069890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac1e781ba0>]}
[0m12:45:59.070468 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:59.070824 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:45:59.071474 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:45:59.071867 [info ] [MainThread]: 
[0m12:45:59.075576 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:45:59.076132 [info ] [Thread-1 (]: 1 of 2 START seed file default.product ......................................... [RUN]
[0m12:45:59.077097 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:45:59.077504 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:45:59.077976 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:45:59.077753 => 12:45:59.077756
[0m12:45:59.078377 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:45:59.126317 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:45:59.126745 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:45:59.127159 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` "2023-11-17 10:00:00"` string,` "2023-11-17 10:00:00"_2` string)
    
    using delta
    
    
    
    
    
  
[0m12:45:59.127561 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:46:03.685900 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`1` bigint,` Apples` string,` 1` bigint,` 100` bigint,` "2023-11-17 10:00:00"` string,` "2023-11-17 10:00:00"_2` string)
    
    using delta
    
    
    
    
    
  
[0m12:46:03.686428 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:03.687470 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Apples` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:46:03.688452 [debug] [Thread-1 (]: Databricks adapter: operation-id: 0eb25096-b016-4ef3-9aaf-425c0928d033
[0m12:46:03.689093 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:45:59.078625 => 12:46:03.688849
[0m12:46:03.689514 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:46:03.689923 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:46:03.690318 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:46:04.003695 [debug] [Thread-1 (]: Runtime Error in seed product (seeds/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:04.004343 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac1e790910>]}
[0m12:46:04.004931 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file default.product ................................. [[31mERROR[0m in 4.93s]
[0m12:46:04.005674 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:46:04.006198 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:46:04.006962 [info ] [Thread-1 (]: 2 of 2 START seed file default.product_category ................................ [RUN]
[0m12:46:04.007985 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:46:04.008424 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:46:04.008901 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:46:04.008678 => 12:46:04.008680
[0m12:46:04.009294 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:46:04.078089 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:46:04.078524 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:46:04.078971 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:46:04.079367 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:46:05.674562 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:46:05.675055 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:05.676010 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Fruits` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:46:05.677286 [debug] [Thread-1 (]: Databricks adapter: operation-id: e74fa244-5dcd-41dc-8ab5-aa34fc258159
[0m12:46:05.677864 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:46:04.009541 => 12:46:05.677634
[0m12:46:05.678260 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:46:05.678726 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:46:05.679098 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:46:05.943985 [debug] [Thread-1 (]: Runtime Error in seed product_category (seeds/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:05.944539 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dde2b049-8703-4694-a0b8-a5edaeb94ef8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac1e793700>]}
[0m12:46:05.945105 [error] [Thread-1 (]: 2 of 2 ERROR loading seed file default.product_category ........................ [[31mERROR[0m in 1.94s]
[0m12:46:05.945931 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:46:05.947677 [debug] [MainThread]: On master: ROLLBACK
[0m12:46:05.948040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:46:06.709044 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:46:06.709504 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:46:06.709853 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:46:06.710192 [debug] [MainThread]: On master: ROLLBACK
[0m12:46:06.710593 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:46:06.710983 [debug] [MainThread]: On master: Close
[0m12:46:06.991001 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:46:06.991512 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:46:06.992246 [info ] [MainThread]: 
[0m12:46:06.992757 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 21.25 seconds (21.25s).
[0m12:46:06.993826 [debug] [MainThread]: Command end result
[0m12:46:07.003211 [info ] [MainThread]: 
[0m12:46:07.003750 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m12:46:07.004185 [info ] [MainThread]: 
[0m12:46:07.004630 [error] [MainThread]:   Runtime Error in seed product (seeds/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:07.005060 [info ] [MainThread]: 
[0m12:46:07.005502 [error] [MainThread]:   Runtime Error in seed product_category (seeds/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:46:07.005935 [info ] [MainThread]: 
[0m12:46:07.006463 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m12:46:07.007493 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 23.972492, "process_user_time": 5.353304, "process_kernel_time": 0.843672, "process_mem_max_rss": "229884", "process_out_blocks": "3448", "command_success": false, "process_in_blocks": "0"}
[0m12:46:07.008072 [debug] [MainThread]: Command `dbt seed` failed at 12:46:07.007941 after 23.97 seconds
[0m12:46:07.008540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac4d189f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24814310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac1e829e40>]}
[0m12:46:07.008958 [debug] [MainThread]: Flushing usage events
[0m12:48:20.920434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f496672b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49646ff640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49646ff5e0>]}


============================== 12:48:20.922817 | c6b82d5f-4fde-4f58-a7fd-d05b413c2036 ==============================
[0m12:48:20.922817 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:48:20.923375 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt seed', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:48:21.893568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49646ff730>]}
[0m12:48:22.034082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4939c4bd00>]}
[0m12:48:22.034673 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:48:22.044492 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:48:22.071541 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:48:22.072151 [debug] [MainThread]: Partial parsing: updated file: databricks_poc://seeds/product.csv
[0m12:48:22.184834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49399d80d0>]}
[0m12:48:22.195469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4939ba6b30>]}
[0m12:48:22.195976 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:48:22.196368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4939ba6b90>]}
[0m12:48:22.197821 [info ] [MainThread]: 
[0m12:48:22.198787 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:48:22.199940 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:48:22.200499 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:48:22.201002 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:48:22.201391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:48:23.294525 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m12:48:23.297692 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:48:23.591712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m12:48:23.592521 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m12:48:23.601016 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:23.601475 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m12:48:23.601856 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m12:48:23.602202 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:48:24.773703 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m12:48:24.774753 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:48:24.775130 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m12:48:24.775576 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:48:24.775895 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m12:48:25.043834 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_default)
[0m12:48:25.048557 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:48:25.048934 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:48:25.049341 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:48:26.156025 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m12:48:26.160949 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:48:26.444443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4939765510>]}
[0m12:48:26.445008 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:26.445350 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:48:26.445999 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:48:26.446384 [info ] [MainThread]: 
[0m12:48:26.449384 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:48:26.449938 [info ] [Thread-1 (]: 1 of 2 START seed file default.product ......................................... [RUN]
[0m12:48:26.450670 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:48:26.451066 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:48:26.451534 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:48:26.451313 => 12:48:26.451315
[0m12:48:26.451919 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:48:26.499231 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:26.499649 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:48:26.500065 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`product_id` bigint,` name` string,` category_id` bigint,` unit_price` bigint,` created_at` string,` updated_at` string)
    
    using delta
    
    
    
    
    
  
[0m12:48:26.500447 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:28.053739 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`product_id` bigint,` name` string,` category_id` bigint,` unit_price` bigint,` created_at` string,` updated_at` string)
    
    using delta
    
    
    
    
    
  
[0m12:48:28.054244 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:28.055254 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` name` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:48:28.056375 [debug] [Thread-1 (]: Databricks adapter: operation-id: 481ed674-4eb9-4134-8b07-62fa722ad490
[0m12:48:28.057161 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:48:26.452167 => 12:48:28.056790
[0m12:48:28.057612 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:48:28.058039 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:48:28.058435 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:48:28.327231 [debug] [Thread-1 (]: Runtime Error in seed product (seeds/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:28.327771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49397741c0>]}
[0m12:48:28.328298 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file default.product ................................. [[31mERROR[0m in 1.88s]
[0m12:48:28.328935 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:48:28.329494 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:48:28.330015 [info ] [Thread-1 (]: 2 of 2 START seed file default.product_category ................................ [RUN]
[0m12:48:28.331040 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:48:28.331448 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:48:28.331948 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:48:28.331727 => 12:48:28.331729
[0m12:48:28.332323 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:48:28.338236 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:28.338598 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:48:28.339037 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:48:28.339460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:29.883547 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`1` bigint,` Fruits` string)
    
    using delta
    
    
    
    
    
  
[0m12:48:29.884043 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:29.885055 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` Fruits` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:48:29.886101 [debug] [Thread-1 (]: Databricks adapter: operation-id: ca44e5a0-3647-4adb-b1d9-46757acbab5e
[0m12:48:29.887698 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:48:28.332567 => 12:48:29.887022
[0m12:48:29.888128 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:48:29.888554 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:48:29.888946 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:48:30.164946 [debug] [Thread-1 (]: Runtime Error in seed product_category (seeds/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:30.165473 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6b82d5f-4fde-4f58-a7fd-d05b413c2036', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49397590c0>]}
[0m12:48:30.166003 [error] [Thread-1 (]: 2 of 2 ERROR loading seed file default.product_category ........................ [[31mERROR[0m in 1.83s]
[0m12:48:30.166618 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:48:30.168361 [debug] [MainThread]: On master: ROLLBACK
[0m12:48:30.168730 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:48:30.920500 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:30.920968 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:48:30.921304 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:48:30.921647 [debug] [MainThread]: On master: ROLLBACK
[0m12:48:30.921979 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:48:30.922303 [debug] [MainThread]: On master: Close
[0m12:48:31.191938 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:48:31.192419 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:48:31.193115 [info ] [MainThread]: 
[0m12:48:31.193641 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 8.99 seconds (8.99s).
[0m12:48:31.194626 [debug] [MainThread]: Command end result
[0m12:48:31.203352 [info ] [MainThread]: 
[0m12:48:31.203936 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m12:48:31.204397 [info ] [MainThread]: 
[0m12:48:31.204916 [error] [MainThread]:   Runtime Error in seed product (seeds/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:31.205373 [info ] [MainThread]: 
[0m12:48:31.205870 [error] [MainThread]:   Runtime Error in seed product_category (seeds/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:48:31.206336 [info ] [MainThread]: 
[0m12:48:31.206956 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m12:48:31.208116 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 10.333828, "process_user_time": 3.876426, "process_kernel_time": 0.7593, "process_mem_max_rss": "221716", "process_out_blocks": "3456", "command_success": false, "process_in_blocks": "0"}
[0m12:48:31.208712 [debug] [MainThread]: Command `dbt seed` failed at 12:48:31.208583 after 10.33 seconds
[0m12:48:31.209179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f496672b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49397741c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f49397655d0>]}
[0m12:48:31.209618 [debug] [MainThread]: Flushing usage events
[0m12:49:44.969361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512cb95f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512ab7b580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512ab7b520>]}


============================== 12:49:44.971571 | 8b1236d8-8307-4df5-b45e-1acae88df3c8 ==============================
[0m12:49:44.971571 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:49:44.972132 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt seed', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:49:45.919319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512ab7b670>]}
[0m12:49:46.055609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f51041015a0>]}
[0m12:49:46.056174 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:49:46.066423 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:49:46.093266 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m12:49:46.093819 [debug] [MainThread]: Partial parsing: added file: databricks_poc://seeds/poc_bronze/product_category.csv
[0m12:49:46.094206 [debug] [MainThread]: Partial parsing: added file: databricks_poc://seeds/poc_bronze/product.csv
[0m12:49:46.094548 [debug] [MainThread]: Partial parsing: deleted file: databricks_poc://seeds/product_category.csv
[0m12:49:46.094875 [debug] [MainThread]: Partial parsing: deleted file: databricks_poc://seeds/product.csv
[0m12:49:46.212375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ffde40d0>]}
[0m12:49:46.223420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50fffaeaa0>]}
[0m12:49:46.224294 [info ] [MainThread]: Found 2 models, 4 tests, 2 seeds, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:49:46.224972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50fffaeb00>]}
[0m12:49:46.227058 [info ] [MainThread]: 
[0m12:49:46.228224 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:49:46.229453 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:49:46.229942 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:49:46.230309 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:49:46.230634 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:47.313712 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m12:49:47.316355 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:49:47.602228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m12:49:47.603105 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m12:49:47.612211 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:49:47.612677 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m12:49:47.613078 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m12:49:47.613471 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:49:48.968993 [debug] [ThreadPool]: SQL status: OK in 1.3600000143051147 seconds
[0m12:49:48.969986 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:49:48.970351 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m12:49:48.970702 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:49:48.971041 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m12:49:49.246852 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_default)
[0m12:49:49.251569 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:49:49.251975 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:49:49.252308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:49:50.346948 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m12:49:50.350272 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:49:50.630010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512aaea6e0>]}
[0m12:49:50.630630 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:49:50.630991 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:49:50.631714 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:49:50.632154 [info ] [MainThread]: 
[0m12:49:50.634981 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:49:50.635553 [info ] [Thread-1 (]: 1 of 2 START seed file default.product ......................................... [RUN]
[0m12:49:50.636379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:49:50.636862 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:49:50.637369 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:49:50.637112 => 12:49:50.637114
[0m12:49:50.637797 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:49:50.686369 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:49:50.686815 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:49:50.687235 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`product_id` bigint,` name` string,` category_id` bigint,` unit_price` bigint,` created_at` string,` updated_at` string)
    
    using delta
    
    
    
    
    
  
[0m12:49:50.687708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:52.290274 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default`.`product` (`product_id` bigint,` name` string,` category_id` bigint,` unit_price` bigint,` created_at` string,` updated_at` string)
    
    using delta
    
    
    
    
    
  
[0m12:49:52.290819 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:52.291789 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` name` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:49:52.292792 [debug] [Thread-1 (]: Databricks adapter: operation-id: 7e74ea2b-dda7-4f5a-886a-4c2608a9f3c5
[0m12:49:52.293409 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:49:50.638038 => 12:49:52.293171
[0m12:49:52.293942 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:49:52.294401 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:49:52.294937 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:49:52.577251 [debug] [Thread-1 (]: Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:52.578104 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ffc76470>]}
[0m12:49:52.578844 [error] [Thread-1 (]: 1 of 2 ERROR loading seed file default.product ................................. [[31mERROR[0m in 1.94s]
[0m12:49:52.579759 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:49:52.580405 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:49:52.581094 [info ] [Thread-1 (]: 2 of 2 START seed file default.product_category ................................ [RUN]
[0m12:49:52.582102 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:49:52.582607 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:49:52.583200 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:49:52.582945 => 12:49:52.582947
[0m12:49:52.583603 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:49:52.589707 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:49:52.590214 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:49:52.590825 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`category_id` bigint,` category_name` string)
    
    using delta
    
    
    
    
    
  
[0m12:49:52.591220 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:54.213135 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default`.`product_category` (`category_id` bigint,` category_name` string)
    
    using delta
    
    
    
    
    
  
[0m12:49:54.213634 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:54.214599 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2072)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2071)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3043)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1017)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:658)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:653)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:623)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:433)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable(OptimisticTransaction.scala:638)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataForNewTable$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataForNewTable(OptimisticTransaction.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:410)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:426)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:217)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:145)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:65)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:131)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:295)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:145)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createTableWithRowColumnControls$1(DeltaCatalog.scala:734)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:104)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTableWithRowColumnControls(DeltaCatalog.scala:705)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:695)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createTable(UnityCatalogV2Proxy.scala:219)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:243)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:56)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:272)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:274)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:498)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:201)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:447)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:271)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:266)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:251)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:251)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:203)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:200)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:565)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column ` category_name` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3114)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1030)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1027)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1027)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1014)
	... 148 more

[0m12:49:54.215608 [debug] [Thread-1 (]: Databricks adapter: operation-id: af23111c-e244-43b6-a9d6-bf27a80b39b9
[0m12:49:54.216196 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:49:52.583862 => 12:49:54.215961
[0m12:49:54.216593 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:49:54.217000 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:49:54.217373 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:49:54.483956 [debug] [Thread-1 (]: Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:54.484504 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b1236d8-8307-4df5-b45e-1acae88df3c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ffc57370>]}
[0m12:49:54.485059 [error] [Thread-1 (]: 2 of 2 ERROR loading seed file default.product_category ........................ [[31mERROR[0m in 1.90s]
[0m12:49:54.485711 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:49:54.487417 [debug] [MainThread]: On master: ROLLBACK
[0m12:49:54.487814 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:49:55.228614 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:49:55.229096 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:49:55.229450 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:49:55.229819 [debug] [MainThread]: On master: ROLLBACK
[0m12:49:55.230179 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:49:55.230529 [debug] [MainThread]: On master: Close
[0m12:49:55.504635 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:49:55.505079 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:49:55.505734 [info ] [MainThread]: 
[0m12:49:55.506165 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 9.28 seconds (9.28s).
[0m12:49:55.507041 [debug] [MainThread]: Command end result
[0m12:49:55.516310 [info ] [MainThread]: 
[0m12:49:55.516819 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m12:49:55.517206 [info ] [MainThread]: 
[0m12:49:55.517661 [error] [MainThread]:   Runtime Error in seed product (seeds/poc_bronze/product.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:55.518107 [info ] [MainThread]: 
[0m12:49:55.518501 [error] [MainThread]:   Runtime Error in seed product_category (seeds/poc_bronze/product_category.csv)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m12:49:55.518906 [info ] [MainThread]: 
[0m12:49:55.519305 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m12:49:55.520130 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 10.593213, "process_user_time": 3.80177, "process_kernel_time": 0.832577, "process_mem_max_rss": "215356", "process_out_blocks": "3456", "command_success": false, "process_in_blocks": "0"}
[0m12:49:55.520723 [debug] [MainThread]: Command `dbt seed` failed at 12:49:55.520579 after 10.59 seconds
[0m12:49:55.521134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f512cb95f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ffc567d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f50ffc55c90>]}
[0m12:49:55.521558 [debug] [MainThread]: Flushing usage events
[0m12:52:41.431760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18becbdf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18bcc5f5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18bcc5f580>]}


============================== 12:52:41.434404 | a8651baa-0b46-4178-a815-a16c49f7ccf9 ==============================
[0m12:52:41.434404 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:52:41.435032 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt seed', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:52:42.468579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18bcc5f6d0>]}
[0m12:52:42.607087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18bcc01930>]}
[0m12:52:42.607710 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:52:42.617662 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:52:42.629114 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:52:42.629687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f189227cfa0>]}
[0m12:52:44.323878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18905040d0>]}
[0m12:52:44.336022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18921a5810>]}
[0m12:52:44.336622 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m12:52:44.337103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1891f9feb0>]}
[0m12:52:44.338759 [info ] [MainThread]: 
[0m12:52:44.339692 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:52:44.341487 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:52:44.342108 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:52:44.342550 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:52:44.342958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:52:45.422217 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m12:52:45.424942 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:52:45.720206 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m12:52:45.721020 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m12:52:45.729518 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:52:45.729913 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m12:52:45.730274 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m12:52:45.730736 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:52:46.927276 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m12:52:46.928321 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:52:46.928717 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m12:52:46.929096 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:52:46.929518 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m12:52:47.197725 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m12:52:47.202482 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m12:52:47.203088 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m12:52:47.203482 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:52:48.322552 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m12:52:48.327122 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m12:52:48.629940 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m12:52:48.632557 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m12:52:48.632977 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m12:52:48.633350 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:52:49.686814 [debug] [ThreadPool]: SQL status: OK in 1.0499999523162842 seconds
[0m12:52:49.689537 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m12:52:49.975040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18903b6530>]}
[0m12:52:49.975623 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:52:49.975978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:52:49.976626 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:52:49.977026 [info ] [MainThread]: 
[0m12:52:49.979440 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:52:49.979960 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product .............................. [RUN]
[0m12:52:49.980725 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product)
[0m12:52:49.981125 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:52:49.981640 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:52:49.981414 => 12:52:49.981417
[0m12:52:49.982018 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:52:50.031408 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:52:50.031848 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:52:50.032284 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`default_poc_bronze`.`product` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at string, updated_at string)
    
    using delta
    
    
    
    
    
  
[0m12:52:50.032695 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:53:06.538552 [debug] [Thread-1 (]: SQL status: OK in 16.510000228881836 seconds
[0m12:53:06.921408 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:53:06.921950 [debug] [Thread-1 (]: On seed.databricks_poc.product: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m12:53:16.407741 [debug] [Thread-1 (]: SQL status: OK in 9.489999771118164 seconds
[0m12:53:16.693110 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product"
[0m12:53:16.709110 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m12:53:16.710142 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:52:49.982262 => 12:53:16.709916
[0m12:53:16.710547 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:53:16.710949 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:53:16.711327 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:53:16.979822 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1890637040>]}
[0m12:53:16.980457 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product .......................... [[32mINSERT 10[0m in 27.00s]
[0m12:53:16.981284 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:53:16.981819 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:53:16.982488 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category ..................... [RUN]
[0m12:53:16.983326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:53:16.983734 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:53:16.984228 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:53:16.983989 => 12:53:16.983991
[0m12:53:16.984629 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:53:16.991004 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:53:16.991437 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:53:16.991853 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m12:53:16.992256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:53:21.088881 [debug] [Thread-1 (]: SQL status: OK in 4.099999904632568 seconds
[0m12:53:21.092320 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:53:21.092806 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m12:53:24.004286 [debug] [Thread-1 (]: SQL status: OK in 2.9100000858306885 seconds
[0m12:53:24.006004 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category"
[0m12:53:24.008524 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m12:53:24.009479 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:53:16.984880 => 12:53:24.009241
[0m12:53:24.009912 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:53:24.010331 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:53:24.010756 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:53:24.293166 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a8651baa-0b46-4178-a815-a16c49f7ccf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18906ca4a0>]}
[0m12:53:24.293789 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category ................. [[32mINSERT 3[0m in 7.31s]
[0m12:53:24.294689 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:53:24.296714 [debug] [MainThread]: On master: ROLLBACK
[0m12:53:24.297194 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:53:25.047573 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:53:25.048009 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:53:25.048447 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:53:25.048812 [debug] [MainThread]: On master: ROLLBACK
[0m12:53:25.049179 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:53:25.049551 [debug] [MainThread]: On master: Close
[0m12:53:25.317310 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:53:25.317752 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:53:25.318365 [info ] [MainThread]: 
[0m12:53:25.318783 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 40.98 seconds (40.98s).
[0m12:53:25.319812 [debug] [MainThread]: Command end result
[0m12:53:25.329127 [info ] [MainThread]: 
[0m12:53:25.329618 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:53:25.330008 [info ] [MainThread]: 
[0m12:53:25.330409 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:53:25.331246 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 43.949066, "process_user_time": 5.713822, "process_kernel_time": 0.932255, "process_mem_max_rss": "227704", "process_in_blocks": "24", "process_out_blocks": "3400"}
[0m12:53:25.331793 [debug] [MainThread]: Command `dbt seed` succeeded at 12:53:25.331657 after 43.95 seconds
[0m12:53:25.332202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18becbdf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18bcc01930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f189056d0f0>]}
[0m12:53:25.332598 [debug] [MainThread]: Flushing usage events
[0m13:03:47.870629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2149a3f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2147a23640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2147a235e0>]}


============================== 13:03:47.872896 | 124e654d-6a1f-4db8-aa9c-70d6d2dc64f8 ==============================
[0m13:03:47.872896 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:03:47.873559 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:03:48.822032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2147a23730>]}
[0m13:03:48.961184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2147a23580>]}
[0m13:03:48.961826 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:03:48.972282 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:03:48.983321 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:03:48.983918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f21211a96f0>]}
[0m13:03:50.504329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f210b1d80d0>]}
[0m13:03:50.515262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2120ced330>]}
[0m13:03:50.515792 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:03:50.516207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '124e654d-6a1f-4db8-aa9c-70d6d2dc64f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2120f317e0>]}
[0m13:03:50.517969 [info ] [MainThread]: 
[0m13:03:50.518823 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:03:50.520156 [debug] [ThreadPool]: Acquiring new databricks connection 'list_poc_bronze'
[0m13:03:50.520790 [debug] [ThreadPool]: Using databricks connection "list_poc_bronze"
[0m13:03:50.521201 [debug] [ThreadPool]: On list_poc_bronze: GetSchemas(database=`poc_bronze`, schema=None)
[0m13:03:50.521651 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:03:51.669637 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m13:03:51.672346 [debug] [ThreadPool]: On list_poc_bronze: Close
[0m13:03:51.936333 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_poc_bronze, now create_poc_bronze_default_hive_metastore)
[0m13:03:51.937336 [debug] [ThreadPool]: Creating schema "database: "poc_bronze"
schema: "default_hive_metastore"
"
[0m13:03:51.945597 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:03:51.945956 [debug] [ThreadPool]: Using databricks connection "create_poc_bronze_default_hive_metastore"
[0m13:03:51.946351 [debug] [ThreadPool]: On create_poc_bronze_default_hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_poc_bronze_default_hive_metastore"} */
create schema if not exists `poc_bronze`.`default_hive_metastore`
  
[0m13:03:51.946699 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:03:53.391868 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_poc_bronze_default_hive_metastore"} */
create schema if not exists `poc_bronze`.`default_hive_metastore`
  
[0m13:03:53.392331 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> poc_bronze.default_hive_metastore)
[0m13:03:53.393035 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [INTERNAL_ERROR] org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> poc_bronze.default_hive_metastore)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> poc_bronze.default_hive_metastore)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:88)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
	at org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:56)
	at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:88)
	at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:74)
	at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:50)
	at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:73)
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidDatabaseNameError(QueryCompilationErrors.scala:912)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$DatabaseNameInSessionCatalog$.unapply(ResolveSessionCatalog.scala:1241)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:421)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:72)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:66)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:286)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:286)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:283)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:266)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:353)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:353)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:353)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:233)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:399)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:225)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:225)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:370)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:179)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:431)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:945)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:431)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:427)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:427)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:172)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:162)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:562)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:03:53.393731 [debug] [ThreadPool]: Databricks adapter: operation-id: 6ad1812a-0602-4cd7-a821-fd89e1660c7f
[0m13:03:53.394148 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m13:03:53.394490 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> poc_bronze.default_hive_metastore)
[0m13:03:53.394877 [debug] [ThreadPool]: On create_poc_bronze_default_hive_metastore: ROLLBACK
[0m13:03:53.395228 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:03:53.395542 [debug] [ThreadPool]: On create_poc_bronze_default_hive_metastore: Close
[0m13:03:53.669317 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:03:53.669715 [debug] [MainThread]: Connection 'create_poc_bronze_default_hive_metastore' was properly closed.
[0m13:03:53.670226 [info ] [MainThread]: 
[0m13:03:53.670698 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 3.15 seconds (3.15s).
[0m13:03:53.671438 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> poc_bronze.default_hive_metastore)
[0m13:03:53.672430 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 5.846514, "process_user_time": 5.012223, "process_kernel_time": 0.84374, "process_mem_max_rss": "221944", "process_out_blocks": "2272", "command_success": false, "process_in_blocks": "0"}
[0m13:03:53.673144 [debug] [MainThread]: Command `dbt seed` failed at 13:03:53.672958 after 5.85 seconds
[0m13:03:53.673634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2149a3f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f210b1d8820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f210b0b65f0>]}
[0m13:03:53.674079 [debug] [MainThread]: Flushing usage events
[0m13:05:07.457060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a01675f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89ff657610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89ff6575b0>]}


============================== 13:05:07.459370 | abed0365-d3d2-4d32-b7be-98b7f0f0a256 ==============================
[0m13:05:07.459370 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:05:07.460022 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:05:08.418065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89ff657700>]}
[0m13:05:08.558296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d8c63d00>]}
[0m13:05:08.558895 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:05:08.568428 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:05:08.579370 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:05:08.579893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d8c62c50>]}
[0m13:05:10.051957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89cb0d00d0>]}
[0m13:05:10.062107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d89e6b30>]}
[0m13:05:10.062600 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:05:10.063054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d8c63880>]}
[0m13:05:10.064639 [info ] [MainThread]: 
[0m13:05:10.065513 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:05:10.066676 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:05:10.067184 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:05:10.067537 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m13:05:10.067875 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:05:11.171768 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m13:05:11.174750 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:05:11.445921 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m13:05:11.446708 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m13:05:11.455304 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:05:11.455666 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m13:05:11.456027 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m13:05:11.456349 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:05:12.818852 [debug] [ThreadPool]: SQL status: OK in 1.3600000143051147 seconds
[0m13:05:12.819843 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:05:12.820207 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m13:05:12.820576 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:05:12.820905 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m13:05:13.102293 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_default)
[0m13:05:13.106941 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:05:13.107331 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m13:05:13.107699 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:05:14.214105 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m13:05:14.217497 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:05:14.495642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89caf85bd0>]}
[0m13:05:14.496283 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:05:14.496639 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:05:14.497293 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:05:14.497691 [info ] [MainThread]: 
[0m13:05:14.500019 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_b
[0m13:05:14.500549 [info ] [Thread-1 (]: 1 of 2 START seed file default.product_b ....................................... [RUN]
[0m13:05:14.501340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product_b)
[0m13:05:14.501729 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_b
[0m13:05:14.502220 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (compile): 13:05:14.501977 => 13:05:14.501979
[0m13:05:14.502597 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_b
[0m13:05:14.550306 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:05:14.550748 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:05:14.551177 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_b"} */

    create table `hive_metastore`.`default`.`product_b` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m13:05:14.551559 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:05:18.284704 [debug] [Thread-1 (]: SQL status: OK in 3.7300000190734863 seconds
[0m13:05:18.378698 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:05:18.379230 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: 
          insert overwrite `hive_metastore`.`default`.`product_b` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m13:05:22.315955 [debug] [Thread-1 (]: SQL status: OK in 3.940000057220459 seconds
[0m13:05:22.321524 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_b"
[0m13:05:22.338936 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:05:22.340006 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (execute): 13:05:14.502839 => 13:05:22.339778
[0m13:05:22.340445 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: ROLLBACK
[0m13:05:22.340837 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:05:22.341258 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: Close
[0m13:05:22.616692 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d8c98940>]}
[0m13:05:22.617311 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default.product_b ................................... [[32mINSERT 10[0m in 8.12s]
[0m13:05:22.617933 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_b
[0m13:05:22.618504 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m13:05:22.619140 [info ] [Thread-1 (]: 2 of 2 START seed file default.product_category_a .............................. [RUN]
[0m13:05:22.620086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_b, now seed.databricks_poc.product_category_a)
[0m13:05:22.620516 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m13:05:22.620996 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 13:05:22.620770 => 13:05:22.620772
[0m13:05:22.621389 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m13:05:22.628013 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:05:22.628455 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:05:22.628864 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m13:05:22.629262 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:05:26.192361 [debug] [Thread-1 (]: SQL status: OK in 3.559999942779541 seconds
[0m13:05:26.195856 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:05:26.201666 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m13:05:28.612263 [debug] [Thread-1 (]: SQL status: OK in 2.4100000858306885 seconds
[0m13:05:28.613420 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m13:05:28.615531 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:05:28.616478 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 13:05:22.621632 => 13:05:28.616242
[0m13:05:28.616913 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m13:05:28.617380 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:05:28.617786 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m13:05:28.891749 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'abed0365-d3d2-4d32-b7be-98b7f0f0a256', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89d8094190>]}
[0m13:05:28.892486 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default.product_category_a .......................... [[32mINSERT 3[0m in 6.27s]
[0m13:05:28.893438 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m13:05:28.895239 [debug] [MainThread]: On master: ROLLBACK
[0m13:05:28.895615 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:05:29.650231 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:05:29.650706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:05:29.651072 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:05:29.651422 [debug] [MainThread]: On master: ROLLBACK
[0m13:05:29.651769 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:05:29.652117 [debug] [MainThread]: On master: Close
[0m13:05:29.917688 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:05:29.918144 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m13:05:29.918838 [info ] [MainThread]: 
[0m13:05:29.919345 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.85 seconds (19.85s).
[0m13:05:29.920472 [debug] [MainThread]: Command end result
[0m13:05:29.930319 [info ] [MainThread]: 
[0m13:05:29.931068 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:05:29.931555 [info ] [MainThread]: 
[0m13:05:29.932152 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:05:29.933080 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 22.519808, "process_user_time": 5.144521, "process_kernel_time": 0.930818, "process_mem_max_rss": "223832", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m13:05:29.933637 [debug] [MainThread]: Command `dbt seed` succeeded at 13:05:29.933508 after 22.52 seconds
[0m13:05:29.934076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a01675f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89ff5c7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89caf8bc70>]}
[0m13:05:29.934500 [debug] [MainThread]: Flushing usage events
[0m13:17:17.514960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274dc65f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274bc4b610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274bc4b5b0>]}


============================== 13:17:17.517861 | a109692a-6a51-4ffd-bbec-0feac4f92662 ==============================
[0m13:17:17.517861 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:17:17.518617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:17:18.495739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274bc4b700>]}
[0m13:17:18.633351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274bbee110>]}
[0m13:17:18.633922 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:17:18.643182 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:17:18.653109 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:17:18.653608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f27253e9840>]}
[0m13:17:20.181834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f270f3dc0d0>]}
[0m13:17:20.192751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2724f0c670>]}
[0m13:17:20.193289 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:17:20.193695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2725155870>]}
[0m13:17:20.195266 [info ] [MainThread]: 
[0m13:17:20.196355 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:17:20.197749 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:17:20.198286 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:17:20.198678 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m13:17:20.199030 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:17:21.411939 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m13:17:21.414516 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:17:21.751650 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m13:17:21.752491 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m13:17:21.760860 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:17:21.761213 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m13:17:21.761635 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m13:17:21.761984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:17:23.542425 [debug] [ThreadPool]: SQL status: OK in 1.7799999713897705 seconds
[0m13:17:23.543426 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:17:23.543805 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m13:17:23.544153 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:17:23.544502 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m13:17:23.819299 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m13:17:23.824239 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m13:17:23.824648 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m13:17:23.825004 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:17:25.054149 [debug] [ThreadPool]: SQL status: OK in 1.2300000190734863 seconds
[0m13:17:25.057664 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m13:17:25.335106 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m13:17:25.338017 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:17:25.338453 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m13:17:25.338811 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:17:26.449928 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m13:17:26.452548 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:17:26.723358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f270f2b2560>]}
[0m13:17:26.724037 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:17:26.724400 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:17:26.725088 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:17:26.725566 [info ] [MainThread]: 
[0m13:17:26.728876 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_b
[0m13:17:26.729469 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product_b ............................ [RUN]
[0m13:17:26.730373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product_b)
[0m13:17:26.730829 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_b
[0m13:17:26.731333 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (compile): 13:17:26.731076 => 13:17:26.731079
[0m13:17:26.731767 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_b
[0m13:17:26.782319 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:17:26.782771 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:17:26.783278 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_b"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_b` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m13:17:26.783667 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:17:30.392554 [debug] [Thread-1 (]: SQL status: OK in 3.609999895095825 seconds
[0m13:17:30.484259 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:17:30.484809 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_b` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m13:17:33.634432 [debug] [Thread-1 (]: SQL status: OK in 3.1500000953674316 seconds
[0m13:17:33.639697 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_b"
[0m13:17:33.654681 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:17:33.655696 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (execute): 13:17:26.732012 => 13:17:33.655476
[0m13:17:33.656121 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: ROLLBACK
[0m13:17:33.656547 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:17:33.656947 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: Close
[0m13:17:33.927188 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f27251bcfa0>]}
[0m13:17:33.927943 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product_b ........................ [[32mINSERT 10[0m in 7.20s]
[0m13:17:33.928770 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_b
[0m13:17:33.929421 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m13:17:33.930190 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category_a ................... [RUN]
[0m13:17:33.931043 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_b, now seed.databricks_poc.product_category_a)
[0m13:17:33.931470 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m13:17:33.931993 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 13:17:33.931739 => 13:17:33.931741
[0m13:17:33.932399 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m13:17:33.938539 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:17:33.938901 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:17:33.939312 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m13:17:33.939720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:17:37.404288 [debug] [Thread-1 (]: SQL status: OK in 3.4600000381469727 seconds
[0m13:17:37.407659 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:17:37.408074 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m13:17:40.341155 [debug] [Thread-1 (]: SQL status: OK in 2.930000066757202 seconds
[0m13:17:40.342240 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m13:17:40.344541 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:17:40.345518 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 13:17:33.932686 => 13:17:40.345271
[0m13:17:40.346006 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m13:17:40.346523 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:17:40.346927 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m13:17:40.648791 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a109692a-6a51-4ffd-bbec-0feac4f92662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f270f5cf430>]}
[0m13:17:40.649448 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category_a ............... [[32mINSERT 3[0m in 6.72s]
[0m13:17:40.650283 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m13:17:40.652187 [debug] [MainThread]: On master: ROLLBACK
[0m13:17:40.652591 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:17:41.411680 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:17:41.412155 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:17:41.412558 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:17:41.412928 [debug] [MainThread]: On master: ROLLBACK
[0m13:17:41.413304 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:17:41.420921 [debug] [MainThread]: On master: Close
[0m13:17:41.698889 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:17:41.699333 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m13:17:41.699975 [info ] [MainThread]: 
[0m13:17:41.700495 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 21.50 seconds (21.50s).
[0m13:17:41.701630 [debug] [MainThread]: Command end result
[0m13:17:41.711045 [info ] [MainThread]: 
[0m13:17:41.711717 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:17:41.712294 [info ] [MainThread]: 
[0m13:17:41.713028 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:17:41.714182 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 24.245878, "process_user_time": 5.338017, "process_kernel_time": 0.874758, "process_mem_max_rss": "231984", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m13:17:41.714778 [debug] [MainThread]: Command `dbt seed` succeeded at 13:17:41.714645 after 24.25 seconds
[0m13:17:41.715194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274dc65f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f274bbee110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f270f46d690>]}
[0m13:17:41.715623 [debug] [MainThread]: Flushing usage events
[0m13:20:30.627119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e14e81f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e12e675e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e12e67580>]}


============================== 13:20:30.629361 | bf7f1eaf-2a48-4aae-b66a-990d961654f1 ==============================
[0m13:20:30.629361 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:20:30.629865 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt seed', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:20:31.646793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e12e676d0>]}
[0m13:20:31.795284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e12e09930>]}
[0m13:20:31.795954 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:20:31.806850 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:20:31.839202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:20:31.839669 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:20:31.846232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec23e920>]}
[0m13:20:31.859344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec306a10>]}
[0m13:20:31.859937 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:20:31.860404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec306c80>]}
[0m13:20:31.862105 [info ] [MainThread]: 
[0m13:20:31.863061 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:20:31.864461 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:20:31.865044 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:20:31.865432 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m13:20:31.865787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:20:33.018423 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m13:20:33.021153 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:20:33.313637 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m13:20:33.314425 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m13:20:33.322910 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:33.323288 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m13:20:33.323638 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m13:20:33.324025 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:20:34.653909 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m13:20:34.654969 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:20:34.655335 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m13:20:34.655682 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:20:34.656024 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m13:20:34.933566 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m13:20:34.938822 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:20:34.939256 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m13:20:34.939589 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:20:36.054874 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m13:20:36.058664 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:20:36.320673 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now list_hive_metastore_default_poc_bronze)
[0m13:20:36.323267 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m13:20:36.323726 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m13:20:36.324054 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:20:37.377156 [debug] [ThreadPool]: SQL status: OK in 1.0499999523162842 seconds
[0m13:20:37.379894 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m13:20:37.669164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec0e7640>]}
[0m13:20:37.669744 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:37.670088 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:20:37.670738 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:20:37.671124 [info ] [MainThread]: 
[0m13:20:37.673851 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_b
[0m13:20:37.674473 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product_b ............................ [RUN]
[0m13:20:37.675450 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now seed.databricks_poc.product_b)
[0m13:20:37.675963 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_b
[0m13:20:37.676463 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (compile): 13:20:37.676220 => 13:20:37.676223
[0m13:20:37.676842 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_b
[0m13:20:37.725312 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:37.725740 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:20:37.726173 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_b"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_b` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m13:20:37.726563 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:20:40.976365 [debug] [Thread-1 (]: SQL status: OK in 3.25 seconds
[0m13:20:41.006907 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_b"
[0m13:20:41.007412 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_b` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m13:20:43.754514 [debug] [Thread-1 (]: SQL status: OK in 2.75 seconds
[0m13:20:43.760819 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_b"
[0m13:20:43.776692 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:20:43.777739 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_b (execute): 13:20:37.677089 => 13:20:43.777520
[0m13:20:43.778167 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: ROLLBACK
[0m13:20:43.778598 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:20:43.779002 [debug] [Thread-1 (]: On seed.databricks_poc.product_b: Close
[0m13:20:44.088913 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec0eece0>]}
[0m13:20:44.089614 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product_b ........................ [[32mINSERT 10[0m in 6.41s]
[0m13:20:44.090507 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_b
[0m13:20:44.091389 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m13:20:44.092075 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category_a ................... [RUN]
[0m13:20:44.092908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_b, now seed.databricks_poc.product_category_a)
[0m13:20:44.093402 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m13:20:44.093918 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 13:20:44.093653 => 13:20:44.093655
[0m13:20:44.094390 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m13:20:44.100310 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:44.100652 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:20:44.101116 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m13:20:44.101679 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:20:47.040628 [debug] [Thread-1 (]: SQL status: OK in 2.940000057220459 seconds
[0m13:20:47.044272 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:20:47.044693 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m13:20:50.007097 [debug] [Thread-1 (]: SQL status: OK in 2.9600000381469727 seconds
[0m13:20:50.008229 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m13:20:50.010524 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:20:50.011486 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 13:20:44.094636 => 13:20:50.011269
[0m13:20:50.012085 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m13:20:50.012511 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:20:50.013032 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m13:20:50.314547 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bf7f1eaf-2a48-4aae-b66a-990d961654f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec3adc90>]}
[0m13:20:50.315180 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category_a ............... [[32mINSERT 3[0m in 6.22s]
[0m13:20:50.315820 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m13:20:50.317332 [debug] [MainThread]: On master: ROLLBACK
[0m13:20:50.317699 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:20:51.062984 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:20:51.063457 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:20:51.063910 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:20:51.064263 [debug] [MainThread]: On master: ROLLBACK
[0m13:20:51.064626 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:20:51.064992 [debug] [MainThread]: On master: Close
[0m13:20:51.335527 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:20:51.335929 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m13:20:51.336666 [info ] [MainThread]: 
[0m13:20:51.337240 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.47 seconds (19.47s).
[0m13:20:51.338676 [debug] [MainThread]: Command end result
[0m13:20:51.347313 [info ] [MainThread]: 
[0m13:20:51.347850 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:20:51.348322 [info ] [MainThread]: 
[0m13:20:51.348875 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:20:51.349806 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 20.764473, "process_user_time": 3.989159, "process_kernel_time": 0.783939, "process_mem_max_rss": "219232", "process_out_blocks": "2296", "process_in_blocks": "0"}
[0m13:20:51.350359 [debug] [MainThread]: Command `dbt seed` succeeded at 13:20:51.350229 after 20.77 seconds
[0m13:20:51.350789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5e14e81f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec2316f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5dec0e7670>]}
[0m13:20:51.351273 [debug] [MainThread]: Flushing usage events
[0m13:30:55.063272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dc0b05f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dbeadb610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dbeadb5b0>]}


============================== 13:30:55.065566 | ff5e126b-a872-47bf-b24c-5fb1e3638ef3 ==============================
[0m13:30:55.065566 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:30:55.066084 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m13:30:56.029994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dbeadb700>]}
[0m13:30:56.173143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dbea7e110>]}
[0m13:30:56.173778 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:30:56.183567 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:30:56.193552 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:30:56.194065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d98279840>]}
[0m13:30:57.750752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d921c40d0>]}
[0m13:30:57.763035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d93c967a0>]}
[0m13:30:57.763612 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:30:57.764070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff5e126b-a872-47bf-b24c-5fb1e3638ef3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d981e1870>]}
[0m13:30:57.765729 [info ] [MainThread]: 
[0m13:30:57.766637 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:30:57.767923 [debug] [ThreadPool]: Acquiring new databricks connection 'list_default'
[0m13:30:57.768446 [debug] [ThreadPool]: Using databricks connection "list_default"
[0m13:30:57.768841 [debug] [ThreadPool]: On list_default: GetSchemas(database=`default`, schema=None)
[0m13:30:57.769190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:30:58.980008 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m13:30:58.982715 [debug] [ThreadPool]: On list_default: Close
[0m13:30:59.277873 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_default, now create_default_default_poc_bronze)
[0m13:30:59.278679 [debug] [ThreadPool]: Creating schema "database: "default"
schema: "default_poc_bronze"
"
[0m13:30:59.287484 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:30:59.287887 [debug] [ThreadPool]: Using databricks connection "create_default_default_poc_bronze"
[0m13:30:59.288242 [debug] [ThreadPool]: On create_default_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_default_default_poc_bronze"} */
create schema if not exists `default`.`default_poc_bronze`
  
[0m13:30:59.288572 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:31:00.550755 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_default_default_poc_bronze"} */
create schema if not exists `default`.`default_poc_bronze`
  
[0m13:31:00.551203 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> default.default_poc_bronze)
[0m13:31:00.551910 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [INTERNAL_ERROR] org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> default.default_poc_bronze)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:607)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:616)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:495)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:493)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> default.default_poc_bronze)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:88)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
	at org.apache.spark.ErrorClassesJsonReader.getErrorMessage(ErrorClassesJSONReader.scala:56)
	at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:88)
	at org.apache.spark.SparkThrowableHelper$.getMessage(SparkThrowableHelper.scala:74)
	at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:50)
	at org.apache.spark.sql.AnalysisException.<init>(AnalysisException.scala:73)
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidDatabaseNameError(QueryCompilationErrors.scala:912)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$DatabaseNameInSessionCatalog$.unapply(ResolveSessionCatalog.scala:1241)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:421)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:72)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:66)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:286)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:286)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:283)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:266)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:353)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:353)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:353)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:233)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:399)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:225)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:225)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:370)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:369)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:179)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:431)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:945)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:431)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:427)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:427)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:172)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:162)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:562)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1113)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:670)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m13:31:00.552620 [debug] [ThreadPool]: Databricks adapter: operation-id: cf814944-017a-408b-a46d-a78e470693c8
[0m13:31:00.553129 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m13:31:00.553478 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> default.default_poc_bronze)
[0m13:31:00.553891 [debug] [ThreadPool]: On create_default_default_poc_bronze: ROLLBACK
[0m13:31:00.554247 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:31:00.554581 [debug] [ThreadPool]: On create_default_default_poc_bronze: Close
[0m13:31:00.912596 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:31:00.913027 [debug] [MainThread]: Connection 'create_default_default_poc_bronze' was properly closed.
[0m13:31:00.913419 [info ] [MainThread]: 
[0m13:31:00.913892 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 3.15 seconds (3.15s).
[0m13:31:00.914490 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    [INTERNAL_ERROR] Undefined error message parameter for error class: '_LEGACY_ERROR_TEMP_1055'. Parameters: Map(database -> default.default_poc_bronze)
[0m13:31:00.915466 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 5.900162, "process_user_time": 4.949738, "process_kernel_time": 0.981931, "process_mem_max_rss": "220264", "process_out_blocks": "2272", "command_success": false, "process_in_blocks": "0"}
[0m13:31:00.916041 [debug] [MainThread]: Command `dbt seed` failed at 13:31:00.915912 after 5.90 seconds
[0m13:31:00.916448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dc0b05f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d921c4820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d9206a5f0>]}
[0m13:31:00.916933 [debug] [MainThread]: Flushing usage events
[0m13:32:35.511078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba70d5f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba6ed43640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba6ed435e0>]}


============================== 13:32:35.513358 | 90e1544a-94cb-49fc-941c-45f4ec17a874 ==============================
[0m13:32:35.513358 [info ] [MainThread]: Running with dbt=1.7.1
[0m13:32:35.513871 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:32:36.441628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba6ed43730>]}
[0m13:32:36.580485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba483f6770>]}
[0m13:32:36.581087 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m13:32:36.590485 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m13:32:36.601134 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:32:36.601716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba48290700>]}
[0m13:32:38.140991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba424d40d0>]}
[0m13:32:38.151262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba48095d50>]}
[0m13:32:38.151781 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 524 macros, 0 groups, 0 semantic models
[0m13:32:38.152194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba482bca30>]}
[0m13:32:38.153933 [info ] [MainThread]: 
[0m13:32:38.154874 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:32:38.156043 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m13:32:38.156542 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:32:38.156916 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m13:32:38.157252 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:39.247633 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m13:32:39.250181 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:32:39.524126 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m13:32:39.525076 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m13:32:39.533626 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:32:39.534007 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m13:32:39.534368 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m13:32:39.534732 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:32:40.830235 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m13:32:40.831320 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:32:40.831715 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m13:32:40.832102 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:32:40.832491 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m13:32:41.102313 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m13:32:41.107099 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m13:32:41.107657 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m13:32:41.108050 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:32:42.197764 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m13:32:42.201606 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m13:32:42.480207 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m13:32:42.482735 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:32:42.483237 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m13:32:42.483565 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:32:43.613020 [debug] [ThreadPool]: SQL status: OK in 1.1299999952316284 seconds
[0m13:32:43.615766 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:32:43.881979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba4239e5c0>]}
[0m13:32:43.882621 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:32:43.883072 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:32:43.883785 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:32:43.884232 [info ] [MainThread]: 
[0m13:32:43.887458 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_a
[0m13:32:43.888041 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product_a ............................ [RUN]
[0m13:32:43.888982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product_a)
[0m13:32:43.889436 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_a
[0m13:32:43.889959 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (compile): 13:32:43.889695 => 13:32:43.889698
[0m13:32:43.890423 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_a
[0m13:32:43.941229 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:32:43.941767 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m13:32:43.942204 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_a` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m13:32:43.942620 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:32:47.541070 [debug] [Thread-1 (]: SQL status: OK in 3.5999999046325684 seconds
[0m13:32:47.633437 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m13:32:47.633997 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_a` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m13:32:50.577947 [debug] [Thread-1 (]: SQL status: OK in 2.940000057220459 seconds
[0m13:32:50.583371 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_a"
[0m13:32:50.598623 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:32:50.599670 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (execute): 13:32:43.890675 => 13:32:50.599447
[0m13:32:50.600101 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: ROLLBACK
[0m13:32:50.600575 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:32:50.600979 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: Close
[0m13:32:50.871471 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba426c2ad0>]}
[0m13:32:50.872128 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product_a ........................ [[32mINSERT 10[0m in 6.98s]
[0m13:32:50.873040 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_a
[0m13:32:50.873890 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m13:32:50.874590 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category_a ................... [RUN]
[0m13:32:50.876027 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_a, now seed.databricks_poc.product_category_a)
[0m13:32:50.876571 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m13:32:50.877114 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 13:32:50.876832 => 13:32:50.876835
[0m13:32:50.877542 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m13:32:50.884666 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:32:50.885022 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:32:50.885521 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m13:32:50.885944 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:32:53.766376 [debug] [Thread-1 (]: SQL status: OK in 2.880000114440918 seconds
[0m13:32:53.769926 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m13:32:53.770379 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m13:32:56.497200 [debug] [Thread-1 (]: SQL status: OK in 2.7300000190734863 seconds
[0m13:32:56.498228 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m13:32:56.501074 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m13:32:56.502088 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 13:32:50.877790 => 13:32:56.501858
[0m13:32:56.502510 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m13:32:56.503035 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:32:56.503422 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m13:32:56.780830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '90e1544a-94cb-49fc-941c-45f4ec17a874', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba426c3400>]}
[0m13:32:56.781458 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category_a ............... [[32mINSERT 3[0m in 5.91s]
[0m13:32:56.782120 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m13:32:56.783908 [debug] [MainThread]: On master: ROLLBACK
[0m13:32:56.784275 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:32:57.530499 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:32:57.530958 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:32:57.531312 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:32:57.531655 [debug] [MainThread]: On master: ROLLBACK
[0m13:32:57.532015 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:32:57.532341 [debug] [MainThread]: On master: Close
[0m13:32:57.803108 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:32:57.803549 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m13:32:57.804148 [info ] [MainThread]: 
[0m13:32:57.804589 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.65 seconds (19.65s).
[0m13:32:57.805734 [debug] [MainThread]: Command end result
[0m13:32:57.815718 [info ] [MainThread]: 
[0m13:32:57.816260 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:32:57.816707 [info ] [MainThread]: 
[0m13:32:57.817162 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:32:57.817979 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 22.348053, "process_user_time": 5.327229, "process_kernel_time": 0.809579, "process_mem_max_rss": "228984", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m13:32:57.818544 [debug] [MainThread]: Command `dbt seed` succeeded at 13:32:57.818406 after 22.35 seconds
[0m13:32:57.818973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba70d5f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba483d0be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba42565090>]}
[0m13:32:57.819371 [debug] [MainThread]: Flushing usage events
[0m14:00:54.992668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e3ff1f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e1fc75e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e1fc7580>]}


============================== 14:00:54.994929 | 4fd71db4-57fe-440c-abaf-9186603d1850 ==============================
[0m14:00:54.994929 [info ] [MainThread]: Running with dbt=1.7.1
[0m14:00:54.995652 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:00:55.992613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e1fc76d0>]}
[0m14:00:56.141623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e1f65930>]}
[0m14:00:56.142307 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m14:00:56.153415 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m14:00:56.186097 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m14:00:56.186713 [debug] [MainThread]: Partial parsing: added file: databricks_poc://macros/generate_schema_name.sql
[0m14:00:56.189001 [info ] [MainThread]: Unable to do partial parsing because change detected to override macro. Starting full parse.
[0m14:00:57.775468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b56d40d0>]}
[0m14:00:57.786033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b7342440>]}
[0m14:00:57.786551 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m14:00:57.786985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b73424a0>]}
[0m14:00:57.788446 [info ] [MainThread]: 
[0m14:00:57.789295 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:00:57.790525 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:00:57.791036 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:00:57.791387 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m14:00:57.791714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:00:58.887254 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m14:00:58.890244 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m14:00:59.184326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m14:00:59.185226 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m14:00:59.254746 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:00:59.255156 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m14:00:59.255527 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m14:00:59.255858 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:01:00.602819 [debug] [ThreadPool]: SQL status: OK in 1.350000023841858 seconds
[0m14:01:00.603902 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:01:00.604302 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m14:01:00.604685 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:01:00.605049 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m14:01:00.888048 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m14:01:00.892751 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m14:01:00.893248 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m14:01:00.893585 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:01:02.045907 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m14:01:02.050037 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m14:01:02.335729 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m14:01:02.338194 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m14:01:02.338584 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m14:01:02.338914 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:01:03.439224 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m14:01:03.442926 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m14:01:03.703888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b58f0880>]}
[0m14:01:03.704546 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:03.704903 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:03.705564 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:01:03.705962 [info ] [MainThread]: 
[0m14:01:03.708445 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_a
[0m14:01:03.708967 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product_a ............................ [RUN]
[0m14:01:03.709737 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product_a)
[0m14:01:03.710132 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_a
[0m14:01:03.711029 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (compile): 14:01:03.710404 => 14:01:03.710407
[0m14:01:03.711463 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_a
[0m14:01:03.761310 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:03.761748 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:01:03.762188 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_a` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m14:01:03.762591 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:01:07.657175 [debug] [Thread-1 (]: SQL status: OK in 3.890000104904175 seconds
[0m14:01:07.681895 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:01:07.682521 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_a` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m14:01:10.017104 [debug] [Thread-1 (]: SQL status: OK in 2.3299999237060547 seconds
[0m14:01:10.022676 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_a"
[0m14:01:10.037926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:01:10.039043 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (execute): 14:01:03.711716 => 14:01:10.038823
[0m14:01:10.039491 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: ROLLBACK
[0m14:01:10.039909 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:01:10.040316 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: Close
[0m14:01:10.326003 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b76da200>]}
[0m14:01:10.326639 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product_a ........................ [[32mINSERT 10[0m in 6.62s]
[0m14:01:10.327274 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_a
[0m14:01:10.327740 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m14:01:10.328372 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category_a ................... [RUN]
[0m14:01:10.329088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_a, now seed.databricks_poc.product_category_a)
[0m14:01:10.329486 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m14:01:10.329985 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 14:01:10.329759 => 14:01:10.329762
[0m14:01:10.330366 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m14:01:10.335915 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:10.336488 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:01:10.337048 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m14:01:10.337477 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:01:13.764384 [debug] [Thread-1 (]: SQL status: OK in 3.430000066757202 seconds
[0m14:01:13.767177 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:01:13.767600 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m14:01:16.140893 [debug] [Thread-1 (]: SQL status: OK in 2.369999885559082 seconds
[0m14:01:16.141884 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m14:01:16.143923 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:01:16.144889 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 14:01:10.330632 => 14:01:16.144675
[0m14:01:16.145315 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m14:01:16.145833 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:01:16.146238 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m14:01:16.425929 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fd71db4-57fe-440c-abaf-9186603d1850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b58f25c0>]}
[0m14:01:16.426574 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category_a ............... [[32mINSERT 3[0m in 6.10s]
[0m14:01:16.427341 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m14:01:16.429549 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:16.429968 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:01:17.172563 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:17.173072 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:17.173501 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:17.173888 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:17.174260 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:17.174597 [debug] [MainThread]: On master: Close
[0m14:01:17.444266 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:01:17.444691 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m14:01:17.445373 [info ] [MainThread]: 
[0m14:01:17.445808 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.66 seconds (19.66s).
[0m14:01:17.446921 [debug] [MainThread]: Command end result
[0m14:01:17.457023 [info ] [MainThread]: 
[0m14:01:17.457746 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:01:17.458360 [info ] [MainThread]: 
[0m14:01:17.458820 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:01:17.459664 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 22.507792, "process_user_time": 5.542796, "process_kernel_time": 0.892059, "process_mem_max_rss": "227192", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m14:01:17.460208 [debug] [MainThread]: Command `dbt seed` succeeded at 14:01:17.460072 after 22.51 seconds
[0m14:01:17.460809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e3ff1f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2b7612d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2e1fc76d0>]}
[0m14:01:17.461245 [debug] [MainThread]: Flushing usage events
[0m14:05:31.415979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df614df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df41335e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df4133580>]}


============================== 14:05:31.418311 | e7cc5feb-5772-4706-aba2-63060da51956 ==============================
[0m14:05:31.418311 [info ] [MainThread]: Running with dbt=1.7.1
[0m14:05:31.418821 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:05:32.430118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7cc5feb-5772-4706-aba2-63060da51956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df41336d0>]}
[0m14:05:32.581919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7cc5feb-5772-4706-aba2-63060da51956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df40d5930>]}
[0m14:05:32.582664 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m14:05:32.592182 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m14:05:32.602176 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:05:32.602711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e7cc5feb-5772-4706-aba2-63060da51956', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc96b2cb0>]}
[0m14:05:33.797034 [error] [MainThread]: Encountered an error:
Parsing Error
  at path ['schema']: None is not of type 'string'
[0m14:05:33.798015 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_wall_clock_time": 2.4293787, "process_user_time": 4.848379, "process_kernel_time": 0.811402, "process_mem_max_rss": "204724", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m14:05:33.798637 [debug] [MainThread]: Command `dbt seed` failed at 14:05:33.798478 after 2.43 seconds
[0m14:05:33.799067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4df614df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc9535450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc9803fa0>]}
[0m14:05:33.799497 [debug] [MainThread]: Flushing usage events
[0m14:08:53.695347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3c50b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3a4ef670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3a4ef610>]}


============================== 14:08:53.697578 | 741e7a4e-5128-4d56-b7b8-a32f9f4bafb5 ==============================
[0m14:08:53.697578 [info ] [MainThread]: Running with dbt=1.7.1
[0m14:08:53.698079 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:08:54.652420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3a4ef760>]}
[0m14:08:54.792007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3a490520>]}
[0m14:08:54.792628 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m14:08:54.801897 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m14:08:54.830527 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m14:08:54.831112 [debug] [MainThread]: Partial parsing: added file: databricks_poc://macros/get_custom_schema.sql
[0m14:08:54.831464 [debug] [MainThread]: Partial parsing: deleted file: databricks_poc://macros/generate_schema_name.sql
[0m14:08:54.833721 [info ] [MainThread]: Unable to do partial parsing because change detected to override macro. Starting full parse.
[0m14:08:56.342666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0dad40d0>]}
[0m14:08:56.353543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0f76e620>]}
[0m14:08:56.354070 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m14:08:56.354471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0f76e680>]}
[0m14:08:56.355851 [info ] [MainThread]: 
[0m14:08:56.356636 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:08:56.357802 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:08:56.358295 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:08:56.358648 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m14:08:56.358994 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:57.448641 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m14:08:57.451280 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m14:08:57.750670 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default_poc_bronze)
[0m14:08:57.751789 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default_poc_bronze"
"
[0m14:08:57.841736 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:08:57.842226 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default_poc_bronze"
[0m14:08:57.842602 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default_poc_bronze"} */
create schema if not exists `hive_metastore`.`default_poc_bronze`
  
[0m14:08:57.842965 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:08:59.460459 [debug] [ThreadPool]: SQL status: OK in 1.6200000047683716 seconds
[0m14:08:59.461706 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:08:59.462440 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: ROLLBACK
[0m14:08:59.463019 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:08:59.463363 [debug] [ThreadPool]: On create_hive_metastore_default_poc_bronze: Close
[0m14:08:59.727786 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default_poc_bronze, now list_hive_metastore_default_poc_bronze)
[0m14:08:59.732568 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default_poc_bronze"
[0m14:08:59.733001 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: GetTables(database=hive_metastore, schema=default_poc_bronze, identifier=None)
[0m14:08:59.733352 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:09:00.849512 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m14:09:00.853076 [debug] [ThreadPool]: On list_hive_metastore_default_poc_bronze: Close
[0m14:09:01.119394 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default_poc_bronze, now list_hive_metastore_default)
[0m14:09:01.121886 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m14:09:01.122254 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m14:09:01.122603 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:09:02.178911 [debug] [ThreadPool]: SQL status: OK in 1.059999942779541 seconds
[0m14:09:02.182362 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m14:09:02.444213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0dcb1cc0>]}
[0m14:09:02.444783 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:02.445134 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:09:02.445810 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:09:02.446202 [info ] [MainThread]: 
[0m14:09:02.448578 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_a
[0m14:09:02.449107 [info ] [Thread-1 (]: 1 of 2 START seed file default_poc_bronze.product_a ............................ [RUN]
[0m14:09:02.449887 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now seed.databricks_poc.product_a)
[0m14:09:02.450289 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_a
[0m14:09:02.451170 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (compile): 14:09:02.450544 => 14:09:02.450547
[0m14:09:02.451587 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_a
[0m14:09:02.500401 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:02.500855 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:09:02.501297 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_a` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m14:09:02.501708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:06.043434 [debug] [Thread-1 (]: SQL status: OK in 3.5399999618530273 seconds
[0m14:09:06.068936 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:09:06.069487 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_a` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m14:09:08.882089 [debug] [Thread-1 (]: SQL status: OK in 2.809999942779541 seconds
[0m14:09:08.887885 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_a"
[0m14:09:08.904137 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:09:08.905238 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (execute): 14:09:02.451846 => 14:09:08.904983
[0m14:09:08.905656 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: ROLLBACK
[0m14:09:08.906079 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:09:08.906469 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: Close
[0m14:09:09.181237 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0dc60700>]}
[0m14:09:09.181981 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file default_poc_bronze.product_a ........................ [[32mINSERT 10[0m in 6.73s]
[0m14:09:09.182675 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_a
[0m14:09:09.183278 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m14:09:09.183965 [info ] [Thread-1 (]: 2 of 2 START seed file default_poc_bronze.product_category_a ................... [RUN]
[0m14:09:09.184766 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_a, now seed.databricks_poc.product_category_a)
[0m14:09:09.185193 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m14:09:09.185714 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 14:09:09.185460 => 14:09:09.185462
[0m14:09:09.186133 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m14:09:09.191657 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:09.192038 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:09:09.192463 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`default_poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m14:09:09.192908 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:12.064984 [debug] [Thread-1 (]: SQL status: OK in 2.869999885559082 seconds
[0m14:09:12.067962 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:09:12.068421 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`default_poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m14:09:14.163619 [debug] [Thread-1 (]: SQL status: OK in 2.0899999141693115 seconds
[0m14:09:14.164555 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m14:09:14.166488 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:09:14.167433 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 14:09:09.186397 => 14:09:14.167209
[0m14:09:14.167838 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m14:09:14.168394 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:09:14.168935 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m14:09:14.452982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '741e7a4e-5128-4d56-b7b8-a32f9f4bafb5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0daa2140>]}
[0m14:09:14.453620 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file default_poc_bronze.product_category_a ............... [[32mINSERT 3[0m in 5.27s]
[0m14:09:14.454287 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m14:09:14.455941 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:14.456342 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:15.210412 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:09:15.211096 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:09:15.211823 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:09:15.212273 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:15.212643 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:09:15.212991 [debug] [MainThread]: On master: Close
[0m14:09:15.478820 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:09:15.479316 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m14:09:15.480118 [info ] [MainThread]: 
[0m14:09:15.480807 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.12 seconds (19.12s).
[0m14:09:15.481860 [debug] [MainThread]: Command end result
[0m14:09:15.492238 [info ] [MainThread]: 
[0m14:09:15.493012 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:09:15.493936 [info ] [MainThread]: 
[0m14:09:15.494802 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:09:15.496110 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 21.842648, "process_user_time": 5.364403, "process_kernel_time": 0.867477, "process_mem_max_rss": "223528", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m14:09:15.496989 [debug] [MainThread]: Command `dbt seed` succeeded at 14:09:15.496691 after 21.84 seconds
[0m14:09:15.497568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3c50b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3a490520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e0db4cd60>]}
[0m14:09:15.498039 [debug] [MainThread]: Flushing usage events
[0m14:15:03.313538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed99bb5f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed97b975b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed97b97550>]}


============================== 14:15:03.315725 | fd6e9aac-9066-4f91-8922-baaea79001cc ==============================
[0m14:15:03.315725 [info ] [MainThread]: Running with dbt=1.7.1
[0m14:15:03.316223 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt seed', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:15:04.291958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed97b976a0>]}
[0m14:15:04.431198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed711a0dc0>]}
[0m14:15:04.431803 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m14:15:04.441190 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m14:15:04.470036 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:15:04.470652 [debug] [MainThread]: Partial parsing: updated file: databricks_poc://macros/get_custom_schema.sql
[0m14:15:04.472857 [info ] [MainThread]: Unable to do partial parsing because change detected to override macro. Starting full parse.
[0m14:15:06.023668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b2c00d0>]}
[0m14:15:06.035464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed70faa560>]}
[0m14:15:06.036049 [info ] [MainThread]: Found 2 models, 2 seeds, 4 tests, 0 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m14:15:06.036523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed70faa5c0>]}
[0m14:15:06.038133 [info ] [MainThread]: 
[0m14:15:06.039043 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:15:06.040338 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m14:15:06.040880 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:15:06.041305 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m14:15:06.041653 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:07.177071 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m14:15:07.179887 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m14:15:07.473422 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_bronze)
[0m14:15:07.474310 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_bronze"
"
[0m14:15:07.546894 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:07.547357 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_bronze"
[0m14:15:07.547737 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_bronze"} */
create schema if not exists `hive_metastore`.`poc_bronze`
  
[0m14:15:07.548094 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:08.725227 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m14:15:08.726242 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:15:08.726604 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: ROLLBACK
[0m14:15:08.726950 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:15:08.727295 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: Close
[0m14:15:09.017316 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_bronze, now list_hive_metastore_default)
[0m14:15:09.022015 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m14:15:09.022426 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m14:15:09.022762 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:10.106428 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m14:15:10.109590 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m14:15:10.377909 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now list_hive_metastore_poc_bronze)
[0m14:15:10.380513 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m14:15:10.380900 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m14:15:10.381253 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:11.491526 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m14:15:11.494665 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m14:15:11.769500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed70e41750>]}
[0m14:15:11.770135 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:11.770495 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:15:11.771189 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:15:11.771621 [info ] [MainThread]: 
[0m14:15:11.774822 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_a
[0m14:15:11.775426 [info ] [Thread-1 (]: 1 of 2 START seed file poc_bronze.product_a .................................... [RUN]
[0m14:15:11.776323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now seed.databricks_poc.product_a)
[0m14:15:11.776745 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_a
[0m14:15:11.777674 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (compile): 14:15:11.777011 => 14:15:11.777014
[0m14:15:11.778264 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_a
[0m14:15:11.827054 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:11.827489 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:15:11.827927 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_a"} */

    create table `hive_metastore`.`poc_bronze`.`product_a` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m14:15:11.828325 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:15.357547 [debug] [Thread-1 (]: SQL status: OK in 3.5299999713897705 seconds
[0m14:15:15.382735 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_a"
[0m14:15:15.383290 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product_a` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m14:15:17.952240 [debug] [Thread-1 (]: SQL status: OK in 2.569999933242798 seconds
[0m14:15:17.957792 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_a"
[0m14:15:17.973416 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:15:17.974508 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_a (execute): 14:15:11.778639 => 14:15:17.974287
[0m14:15:17.974940 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: ROLLBACK
[0m14:15:17.975378 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:15:17.975800 [debug] [Thread-1 (]: On seed.databricks_poc.product_a: Close
[0m14:15:18.260723 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed71006ad0>]}
[0m14:15:18.261346 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file poc_bronze.product_a ................................ [[32mINSERT 10[0m in 6.48s]
[0m14:15:18.262007 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_a
[0m14:15:18.262661 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category_a
[0m14:15:18.263631 [info ] [Thread-1 (]: 2 of 2 START seed file poc_bronze.product_category_a ........................... [RUN]
[0m14:15:18.264503 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product_a, now seed.databricks_poc.product_category_a)
[0m14:15:18.264960 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category_a
[0m14:15:18.265473 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (compile): 14:15:18.265220 => 14:15:18.265222
[0m14:15:18.265870 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category_a
[0m14:15:18.271260 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:18.271627 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:15:18.272095 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category_a"} */

    create table `hive_metastore`.`poc_bronze`.`product_category_a` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m14:15:18.272503 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:21.742331 [debug] [Thread-1 (]: SQL status: OK in 3.4700000286102295 seconds
[0m14:15:21.745193 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category_a"
[0m14:15:21.745611 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product_category_a` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m14:15:24.014472 [debug] [Thread-1 (]: SQL status: OK in 2.2699999809265137 seconds
[0m14:15:24.015491 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category_a"
[0m14:15:24.017598 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m14:15:24.018626 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category_a (execute): 14:15:18.266124 => 14:15:24.018333
[0m14:15:24.019094 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: ROLLBACK
[0m14:15:24.019521 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:15:24.019988 [debug] [Thread-1 (]: On seed.databricks_poc.product_category_a: Close
[0m14:15:24.293134 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd6e9aac-9066-4f91-8922-baaea79001cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed70134a30>]}
[0m14:15:24.293770 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file poc_bronze.product_category_a ....................... [[32mINSERT 3[0m in 6.03s]
[0m14:15:24.294448 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category_a
[0m14:15:24.296292 [debug] [MainThread]: On master: ROLLBACK
[0m14:15:24.296689 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:15:25.059253 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:15:25.059755 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:15:25.060094 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:15:25.060441 [debug] [MainThread]: On master: ROLLBACK
[0m14:15:25.060792 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:15:25.061112 [debug] [MainThread]: On master: Close
[0m14:15:25.341803 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:25.342218 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category_a' was properly closed.
[0m14:15:25.342918 [info ] [MainThread]: 
[0m14:15:25.343333 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 19.30 seconds (19.30s).
[0m14:15:25.344346 [debug] [MainThread]: Command end result
[0m14:15:25.353606 [info ] [MainThread]: 
[0m14:15:25.354088 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:15:25.354494 [info ] [MainThread]: 
[0m14:15:25.354980 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:15:25.355882 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 22.083649, "process_user_time": 5.500492, "process_kernel_time": 0.780069, "process_mem_max_rss": "223364", "process_out_blocks": "3400", "process_in_blocks": "0"}
[0m14:15:25.356411 [debug] [MainThread]: Command `dbt seed` succeeded at 14:15:25.356276 after 22.08 seconds
[0m14:15:25.356811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed99bb5f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed713be410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed70e4ba00>]}
[0m14:15:25.357206 [debug] [MainThread]: Flushing usage events
[0m21:36:43.433172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40b9325f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40b73135b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40b7313550>]}


============================== 21:36:43.436133 | 9d1eac51-67b8-4ded-bcfb-7977c7accefc ==============================
[0m21:36:43.436133 [info ] [MainThread]: Running with dbt=1.7.1
[0m21:36:43.436769 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt seed', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:36:44.824339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40b73136a0>]}
[0m21:36:45.034352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409088cdc0>]}
[0m21:36:45.035135 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m21:36:45.059191 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m21:36:45.070796 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m21:36:45.071402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409088c9d0>]}
[0m21:36:47.217011 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.databricks_poc.example
[0m21:36:47.233855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40905080d0>]}
[0m21:36:47.255879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4090548e20>]}
[0m21:36:47.256413 [info ] [MainThread]: Found 2 seeds, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m21:36:47.257202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409047dff0>]}
[0m21:36:47.261529 [info ] [MainThread]: 
[0m21:36:47.265667 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:36:47.272358 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:36:47.273108 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:36:47.273564 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m21:36:47.273913 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:36:49.220937 [debug] [ThreadPool]: SQL status: OK in 1.9500000476837158 seconds
[0m21:36:49.224422 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m21:36:49.733449 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_bronze)
[0m21:36:49.734419 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_bronze"
"
[0m21:36:49.743953 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:36:49.744385 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_bronze"
[0m21:36:49.744802 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_bronze"} */
create schema if not exists `hive_metastore`.`poc_bronze`
  
[0m21:36:49.745191 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:51.511176 [debug] [ThreadPool]: SQL status: OK in 1.7699999809265137 seconds
[0m21:36:51.512189 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:36:51.512565 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: ROLLBACK
[0m21:36:51.512953 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:36:51.513294 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: Close
[0m21:36:51.771110 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_bronze, now list_hive_metastore_poc_bronze)
[0m21:36:51.776051 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m21:36:51.776494 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m21:36:51.776830 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:36:52.891486 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m21:36:52.898956 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m21:36:53.156273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40903c14b0>]}
[0m21:36:53.156834 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:36:53.157183 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:36:53.157984 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:36:53.158390 [info ] [MainThread]: 
[0m21:36:53.161429 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m21:36:53.161987 [info ] [Thread-1 (]: 1 of 2 START seed file poc_bronze.product ...................................... [RUN]
[0m21:36:53.163180 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now seed.databricks_poc.product)
[0m21:36:53.163681 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m21:36:53.164182 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 21:36:53.163952 => 21:36:53.163955
[0m21:36:53.164590 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m21:36:53.249921 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:36:53.250434 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m21:36:53.251100 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`poc_bronze`.`product` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m21:36:53.251510 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:37:01.497886 [debug] [Thread-1 (]: SQL status: OK in 8.25 seconds
[0m21:37:01.820479 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m21:37:01.821085 [debug] [Thread-1 (]: On seed.databricks_poc.product: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m21:37:10.935144 [debug] [Thread-1 (]: SQL status: OK in 9.109999656677246 seconds
[0m21:37:11.199183 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product"
[0m21:37:11.246542 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:37:11.247725 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 21:36:53.164842 => 21:37:11.247480
[0m21:37:11.248153 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m21:37:11.248582 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:37:11.248974 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m21:37:11.513693 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40903cece0>]}
[0m21:37:11.514354 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file poc_bronze.product .................................. [[32mINSERT 10[0m in 18.35s]
[0m21:37:11.515155 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m21:37:11.516033 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m21:37:11.517048 [info ] [Thread-1 (]: 2 of 2 START seed file poc_bronze.product_category ............................. [RUN]
[0m21:37:11.518240 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m21:37:11.518714 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m21:37:11.519344 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 21:37:11.518990 => 21:37:11.518993
[0m21:37:11.519755 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m21:37:11.525478 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:37:11.525915 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m21:37:11.526385 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`poc_bronze`.`product_category` (category_id bigint, category_name string)
    
    using delta
    
    
    
    
    
  
[0m21:37:11.526794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:37:15.546475 [debug] [Thread-1 (]: SQL status: OK in 4.019999980926514 seconds
[0m21:37:15.558241 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m21:37:15.560537 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product_category` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m21:37:18.182185 [debug] [Thread-1 (]: SQL status: OK in 2.619999885559082 seconds
[0m21:37:18.183206 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category"
[0m21:37:18.189674 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:37:18.195209 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 21:37:11.520016 => 21:37:18.194024
[0m21:37:18.197493 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m21:37:18.198071 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:37:18.198745 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m21:37:18.530552 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9d1eac51-67b8-4ded-bcfb-7977c7accefc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40903b5cf0>]}
[0m21:37:18.533030 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file poc_bronze.product_category ......................... [[32mINSERT 3[0m in 7.01s]
[0m21:37:18.534861 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m21:37:18.543674 [debug] [MainThread]: On master: ROLLBACK
[0m21:37:18.545634 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:37:19.293560 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:37:19.295666 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:37:19.297531 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:37:19.299541 [debug] [MainThread]: On master: ROLLBACK
[0m21:37:19.300556 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:37:19.300978 [debug] [MainThread]: On master: Close
[0m21:37:19.567551 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:37:19.569774 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m21:37:19.572742 [info ] [MainThread]: 
[0m21:37:19.575315 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 32.31 seconds (32.31s).
[0m21:37:19.580103 [debug] [MainThread]: Command end result
[0m21:37:19.623002 [info ] [MainThread]: 
[0m21:37:19.625424 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:37:19.627748 [info ] [MainThread]: 
[0m21:37:19.629949 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:37:19.633653 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 36.267475, "process_user_time": 7.593919, "process_kernel_time": 0.934172, "process_mem_max_rss": "221216", "process_out_blocks": "3328", "process_in_blocks": "0"}
[0m21:37:19.634254 [debug] [MainThread]: Command `dbt seed` succeeded at 21:37:19.634116 after 36.27 seconds
[0m21:37:19.634760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f40b9325f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4090aa2410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f409088cdc0>]}
[0m21:37:19.635738 [debug] [MainThread]: Flushing usage events
[0m00:16:31.846044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f579b641f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57995f3610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57995f35b0>]}


============================== 00:16:31.849294 | b4bfa63f-6878-445d-9cdb-78e1f29ab084 ==============================
[0m00:16:31.849294 [info ] [MainThread]: Running with dbt=1.7.1
[0m00:16:31.850096 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:16:33.128746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4bfa63f-6878-445d-9cdb-78e1f29ab084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57995f3700>]}
[0m00:16:33.372486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4bfa63f-6878-445d-9cdb-78e1f29ab084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5799592110>]}
[0m00:16:33.373286 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m00:16:33.386378 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m00:16:33.398825 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m00:16:33.399453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b4bfa63f-6878-445d-9cdb-78e1f29ab084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f576ee38e80>]}
[0m00:16:33.403420 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading databricks_poc: sources.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | sources:
    4  |     - name: bronze_db
    5  |         database: hive_metastore  
    6  |         schema: poc_bronze
    7  |         tables:
    8  |             - name: product
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 5, column 17
[0m00:16:33.404749 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.6145031, "process_user_time": 4.52838, "process_kernel_time": 0.863504, "process_mem_max_rss": "205264", "process_out_blocks": "16", "command_success": false, "process_in_blocks": "0"}
[0m00:16:33.405519 [debug] [MainThread]: Command `dbt run` failed at 00:16:33.405311 after 1.62 seconds
[0m00:16:33.406073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f579b641f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f576ed24490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f576ec390f0>]}
[0m00:16:33.406635 [debug] [MainThread]: Flushing usage events
[0m00:25:59.943558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe704541f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe70252f5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe70252f580>]}


============================== 00:25:59.946520 | af6e1196-37e6-48bc-aefc-a4a8e67f37dc ==============================
[0m00:25:59.946520 [info ] [MainThread]: Running with dbt=1.7.1
[0m00:25:59.947279 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:26:01.429204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe70252f6d0>]}
[0m00:26:01.637850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7024d5930>]}
[0m00:26:01.638608 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m00:26:01.652036 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m00:26:01.664708 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m00:26:01.665388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d7a3ecb0>]}
[0m00:26:03.754980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d60392a0>]}
[0m00:26:03.768506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d634ec80>]}
[0m00:26:03.769210 [info ] [MainThread]: Found 1 model, 2 seeds, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m00:26:03.769884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d79a6ce0>]}
[0m00:26:03.771993 [info ] [MainThread]: 
[0m00:26:03.773353 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:26:03.775195 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:26:03.776192 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:26:03.776849 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m00:26:03.777558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:27:12.392228 [debug] [ThreadPool]: SQL status: OK in 68.61000061035156 seconds
[0m00:27:12.403457 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m00:27:12.852115 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_default)
[0m00:27:12.855937 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "default"
"
[0m00:27:12.896669 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:27:12.898413 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_default"
[0m00:27:12.900240 [debug] [ThreadPool]: On create_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_default"} */
create schema if not exists `hive_metastore`.`default`
  
[0m00:27:12.902092 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:19.010158 [debug] [ThreadPool]: SQL status: OK in 6.110000133514404 seconds
[0m00:27:19.019750 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:27:19.024131 [debug] [ThreadPool]: On create_hive_metastore_default: ROLLBACK
[0m00:27:19.027818 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:27:19.029680 [debug] [ThreadPool]: On create_hive_metastore_default: Close
[0m00:27:19.328961 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_default, now list_hive_metastore_poc_bronze)
[0m00:27:19.350941 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:27:19.352893 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m00:27:19.354942 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:21.000593 [debug] [ThreadPool]: SQL status: OK in 1.649999976158142 seconds
[0m00:27:21.039968 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:27:21.041768 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:27:21.043768 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m00:27:23.959449 [debug] [ThreadPool]: SQL status: OK in 2.9100000858306885 seconds
[0m00:27:24.158238 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:27:24.159397 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m00:27:24.843117 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m00:27:24.877838 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:27:24.880004 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m00:27:28.261423 [debug] [ThreadPool]: SQL status: OK in 3.380000114440918 seconds
[0m00:27:28.282338 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m00:27:28.284311 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:27:28.285953 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m00:27:28.770814 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now list_hive_metastore_default)
[0m00:27:28.784386 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m00:27:28.786332 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m00:27:28.788022 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:27:30.133869 [debug] [ThreadPool]: SQL status: OK in 1.350000023841858 seconds
[0m00:27:30.146592 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m00:27:30.466463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d60df040>]}
[0m00:27:30.468951 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:27:30.470845 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:27:30.474499 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:27:30.476549 [info ] [MainThread]: 
[0m00:27:30.486594 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_fact
[0m00:27:30.489625 [info ] [Thread-1 (]: 1 of 1 START sql view model default.product_fact ............................... [RUN]
[0m00:27:30.493925 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.databricks_poc.product_fact)
[0m00:27:30.496321 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_fact
[0m00:27:30.535742 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_fact"
[0m00:27:30.543471 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (compile): 00:27:30.497731 => 00:27:30.541711
[0m00:27:30.545752 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_fact
[0m00:27:30.607525 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_fact"
[0m00:27:30.609077 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:27:30.609576 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_fact"
[0m00:27:30.610234 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_fact"} */
create or replace view `hive_metastore`.`default`.`product_fact`
  
  
  as
    SELECT 
    product_id, name
FROM `hive_metastore`.`poc_bronze`.`product`

[0m00:27:30.610827 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:27:33.481108 [debug] [Thread-1 (]: SQL status: OK in 2.869999885559082 seconds
[0m00:27:33.523182 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (execute): 00:27:30.547149 => 00:27:33.522595
[0m00:27:33.524316 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: ROLLBACK
[0m00:27:33.525356 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:27:33.526299 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: Close
[0m00:27:33.786862 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af6e1196-37e6-48bc-aefc-a4a8e67f37dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d624c6a0>]}
[0m00:27:33.787880 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.product_fact .......................... [[32mOK[0m in 3.30s]
[0m00:27:33.788950 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_fact
[0m00:27:33.791286 [debug] [MainThread]: On master: ROLLBACK
[0m00:27:33.791925 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:27:34.896974 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:27:34.898868 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:27:34.900603 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:27:34.902391 [debug] [MainThread]: On master: ROLLBACK
[0m00:27:34.904357 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:27:34.906057 [debug] [MainThread]: On master: Close
[0m00:27:35.322521 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:27:35.324554 [debug] [MainThread]: Connection 'model.databricks_poc.product_fact' was properly closed.
[0m00:27:35.327306 [info ] [MainThread]: 
[0m00:27:35.329613 [info ] [MainThread]: Finished running 1 view model in 0 hours 1 minutes and 31.55 seconds (91.55s).
[0m00:27:35.333523 [debug] [MainThread]: Command end result
[0m00:27:35.370021 [info ] [MainThread]: 
[0m00:27:35.372351 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:27:35.374435 [info ] [MainThread]: 
[0m00:27:35.376734 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:27:35.380500 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 95.492256, "process_user_time": 7.913455, "process_kernel_time": 0.951618, "process_mem_max_rss": "228812", "process_in_blocks": "208", "process_out_blocks": "3352"}
[0m00:27:35.383723 [debug] [MainThread]: Command `dbt run` succeeded at 00:27:35.382958 after 95.50 seconds
[0m00:27:35.385872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe704541f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d7b0ed10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6d60396f0>]}
[0m00:27:35.388013 [debug] [MainThread]: Flushing usage events
[0m00:37:19.597247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f401f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9d3ef5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9d3ef580>]}


============================== 00:37:19.600481 | f167bbc4-d340-4982-8c4d-a4a8d9fb3224 ==============================
[0m00:37:19.600481 [info ] [MainThread]: Running with dbt=1.7.1
[0m00:37:19.601245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:37:20.993434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9d3ef6d0>]}
[0m00:37:21.189173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9d395930>]}
[0m00:37:21.189969 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m00:37:21.202917 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m00:37:21.215921 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m00:37:21.216608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a7295c820>]}
[0m00:37:23.138086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a70b612a0>]}
[0m00:37:23.151748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a726836a0>]}
[0m00:37:23.152428 [info ] [MainThread]: Found 1 model, 2 seeds, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m00:37:23.153013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a7274ebc0>]}
[0m00:37:23.154987 [info ] [MainThread]: 
[0m00:37:23.156253 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:37:23.158104 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:37:23.159075 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:37:23.160347 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m00:37:23.161295 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:37:24.542550 [debug] [ThreadPool]: SQL status: OK in 1.3799999952316284 seconds
[0m00:37:24.555526 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m00:37:24.948172 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_silver)
[0m00:37:24.951530 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_silver"
"
[0m00:37:24.988985 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:24.990627 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_silver"
[0m00:37:24.992620 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_silver"} */
create schema if not exists `hive_metastore`.`poc_silver`
  
[0m00:37:24.994442 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:37:26.186856 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m00:37:26.190974 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:37:26.192958 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: ROLLBACK
[0m00:37:26.195184 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:37:26.196880 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: Close
[0m00:37:26.455637 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_silver, now list_hive_metastore_poc_silver)
[0m00:37:26.464004 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m00:37:26.464764 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m00:37:26.465378 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:37:27.628873 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m00:37:27.649748 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m00:37:27.937687 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now list_hive_metastore_poc_bronze)
[0m00:37:27.954410 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:37:27.956403 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m00:37:27.958111 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:37:29.345636 [debug] [ThreadPool]: SQL status: OK in 1.3899999856948853 seconds
[0m00:37:29.464431 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:29.465355 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:37:29.466231 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m00:37:29.893850 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m00:37:29.906657 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:37:29.907428 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m00:37:30.279897 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m00:37:30.310837 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m00:37:30.312959 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m00:37:30.766657 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m00:37:30.777541 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m00:37:30.779243 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:37:30.780709 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m00:37:31.195041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a72682350>]}
[0m00:37:31.197704 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:31.205687 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:31.209352 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:37:31.211456 [info ] [MainThread]: 
[0m00:37:31.221215 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_fact
[0m00:37:31.223805 [info ] [Thread-1 (]: 1 of 1 START sql view model poc_silver.product_fact ............................ [RUN]
[0m00:37:31.227620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now model.databricks_poc.product_fact)
[0m00:37:31.229374 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_fact
[0m00:37:31.264493 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_fact"
[0m00:37:31.267554 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (compile): 00:37:31.230483 => 00:37:31.266422
[0m00:37:31.269585 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_fact
[0m00:37:31.348026 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_fact"
[0m00:37:31.349258 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:31.349822 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_fact"
[0m00:37:31.350555 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_fact"} */
create or replace view `hive_metastore`.`poc_silver`.`product_fact`
  
  
  as
    SELECT 
    product_id, name
FROM `hive_metastore`.`poc_bronze`.`product`

[0m00:37:31.351203 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m00:37:33.116784 [debug] [Thread-1 (]: SQL status: OK in 1.7699999809265137 seconds
[0m00:37:33.165925 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (execute): 00:37:31.270963 => 00:37:33.164914
[0m00:37:33.167783 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: ROLLBACK
[0m00:37:33.169671 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m00:37:33.171602 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: Close
[0m00:37:33.434560 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f167bbc4-d340-4982-8c4d-a4a8d9fb3224', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a728d2c80>]}
[0m00:37:33.437913 [info ] [Thread-1 (]: 1 of 1 OK created sql view model poc_silver.product_fact ....................... [[32mOK[0m in 2.21s]
[0m00:37:33.441900 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_fact
[0m00:37:33.450205 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:33.452624 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:37:34.231442 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:34.233410 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:37:34.235327 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:37:34.237131 [debug] [MainThread]: On master: ROLLBACK
[0m00:37:34.239000 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:37:34.240678 [debug] [MainThread]: On master: Close
[0m00:37:34.503357 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:37:34.505108 [debug] [MainThread]: Connection 'model.databricks_poc.product_fact' was properly closed.
[0m00:37:34.507793 [info ] [MainThread]: 
[0m00:37:34.509924 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 11.35 seconds (11.35s).
[0m00:37:34.513802 [debug] [MainThread]: Command end result
[0m00:37:34.552195 [info ] [MainThread]: 
[0m00:37:34.554694 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:37:34.556670 [info ] [MainThread]: 
[0m00:37:34.558844 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:37:34.562905 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.020784, "process_user_time": 7.458282, "process_kernel_time": 0.907357, "process_mem_max_rss": "228680", "process_out_blocks": "3336", "process_in_blocks": "0"}
[0m00:37:34.565812 [debug] [MainThread]: Command `dbt run` succeeded at 00:37:34.565083 after 15.02 seconds
[0m00:37:34.567826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9f401f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a7270ab90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a9d395930>]}
[0m00:37:34.570306 [debug] [MainThread]: Flushing usage events
[0m12:24:43.622040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1775e0f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1773de7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1773de75e0>]}


============================== 12:24:43.633782 | c7cd588d-feff-4fdb-b204-0862af09592b ==============================
[0m12:24:43.633782 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:24:43.636658 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'debug': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt seed', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:24:48.545039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1773de7730>]}
[0m12:24:49.009872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1773d86020>]}
[0m12:24:49.010478 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:24:49.054544 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:24:49.281084 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:24:49.283835 [debug] [MainThread]: Partial parsing: updated file: databricks_poc://models/silver/product_fact.sql
[0m12:24:49.286492 [debug] [MainThread]: Partial parsing: updated file: databricks_poc://seeds/poc_bronze/product_category.csv
[0m12:24:50.079079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17481d0250>]}
[0m12:24:50.124164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1749161f00>]}
[0m12:24:50.124723 [info ] [MainThread]: Found 2 seeds, 1 model, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m12:24:50.126139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1749162050>]}
[0m12:24:50.134113 [info ] [MainThread]: 
[0m12:24:50.137958 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:24:50.142794 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:24:50.145894 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:24:50.148497 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:24:50.150383 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:24:52.514156 [debug] [ThreadPool]: SQL status: OK in 2.359999895095825 seconds
[0m12:24:52.545723 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:24:53.039003 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_bronze)
[0m12:24:53.041786 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_bronze"
"
[0m12:24:53.068919 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:24:53.069351 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_bronze"
[0m12:24:53.069736 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_bronze"} */
create schema if not exists `hive_metastore`.`poc_bronze`
  
[0m12:24:53.070077 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:24:55.131595 [debug] [ThreadPool]: SQL status: OK in 2.059999942779541 seconds
[0m12:24:55.137748 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:24:55.139706 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: ROLLBACK
[0m12:24:55.141691 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:24:55.142341 [debug] [ThreadPool]: On create_hive_metastore_poc_bronze: Close
[0m12:24:55.462233 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_bronze, now list_hive_metastore_poc_bronze)
[0m12:24:55.476850 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:24:55.477300 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m12:24:55.477648 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:24:56.678560 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m12:24:56.691818 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m12:24:56.962712 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now list_hive_metastore_poc_silver)
[0m12:24:56.972010 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:24:56.974014 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m12:24:56.975933 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:24:58.081381 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m12:24:58.087983 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m12:24:58.368736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17491a4430>]}
[0m12:24:58.371204 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:24:58.373088 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:24:58.376550 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:24:58.377791 [info ] [MainThread]: 
[0m12:24:58.393687 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product
[0m12:24:58.396596 [info ] [Thread-1 (]: 1 of 2 START seed file poc_bronze.product ...................................... [RUN]
[0m12:24:58.400782 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now seed.databricks_poc.product)
[0m12:24:58.403228 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product
[0m12:24:58.405883 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (compile): 12:24:58.404644 => 12:24:58.404656
[0m12:24:58.406493 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product
[0m12:24:58.560811 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:24:58.561411 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:24:58.561895 [debug] [Thread-1 (]: On seed.databricks_poc.product: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product"} */

    create table `hive_metastore`.`poc_bronze`.`product` (product_id bigint, name string, category_id bigint, unit_price bigint, created_at timestamp, updated_at timestamp)
    
    using delta
    
    
    
    
    
  
[0m12:24:58.562310 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:07.719641 [debug] [Thread-1 (]: SQL status: OK in 9.15999984741211 seconds
[0m12:25:08.095249 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product"
[0m12:25:08.097851 [debug] [Thread-1 (]: On seed.databricks_poc.product: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product` values
          (%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s),(%s,%s,%s,%s,%s,%s)
      ...
[0m12:25:18.168548 [debug] [Thread-1 (]: SQL status: OK in 10.069999694824219 seconds
[0m12:25:18.429752 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product"
[0m12:25:18.494310 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m12:25:18.495445 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product (execute): 12:24:58.406754 => 12:25:18.495217
[0m12:25:18.498767 [debug] [Thread-1 (]: On seed.databricks_poc.product: ROLLBACK
[0m12:25:18.499322 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:25:18.499708 [debug] [Thread-1 (]: On seed.databricks_poc.product: Close
[0m12:25:18.753867 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f177744fd30>]}
[0m12:25:18.754494 [info ] [Thread-1 (]: 1 of 2 OK loaded seed file poc_bronze.product .................................. [[32mINSERT 10[0m in 20.36s]
[0m12:25:18.755207 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product
[0m12:25:18.755826 [debug] [Thread-1 (]: Began running node seed.databricks_poc.product_category
[0m12:25:18.756627 [info ] [Thread-1 (]: 2 of 2 START seed file poc_bronze.product_category ............................. [RUN]
[0m12:25:18.757470 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.databricks_poc.product, now seed.databricks_poc.product_category)
[0m12:25:18.757876 [debug] [Thread-1 (]: Began compiling node seed.databricks_poc.product_category
[0m12:25:18.758417 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (compile): 12:25:18.758132 => 12:25:18.758135
[0m12:25:18.758803 [debug] [Thread-1 (]: Began executing node seed.databricks_poc.product_category
[0m12:25:18.766060 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:25:18.766511 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:25:18.766936 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "seed.databricks_poc.product_category"} */

    create table `hive_metastore`.`poc_bronze`.`product_category` (category_id bigint, name string)
    
    using delta
    
    
    
    
    
  
[0m12:25:18.767328 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:23.351437 [debug] [Thread-1 (]: SQL status: OK in 4.579999923706055 seconds
[0m12:25:23.358274 [debug] [Thread-1 (]: Using databricks connection "seed.databricks_poc.product_category"
[0m12:25:23.360762 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: 
          insert overwrite `hive_metastore`.`poc_bronze`.`product_category` values
          (%s,%s),(%s,%s),(%s,%s)
      ...
[0m12:25:27.338745 [debug] [Thread-1 (]: SQL status: OK in 3.9800000190734863 seconds
[0m12:25:27.339719 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.databricks_poc.product_category"
[0m12:25:27.341952 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m12:25:27.342954 [debug] [Thread-1 (]: Timing info for seed.databricks_poc.product_category (execute): 12:25:18.759088 => 12:25:27.342706
[0m12:25:27.343380 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: ROLLBACK
[0m12:25:27.345109 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:25:27.347112 [debug] [Thread-1 (]: On seed.databricks_poc.product_category: Close
[0m12:25:27.617827 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c7cd588d-feff-4fdb-b204-0862af09592b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f173b7a3910>]}
[0m12:25:27.618517 [info ] [Thread-1 (]: 2 of 2 OK loaded seed file poc_bronze.product_category ......................... [[32mINSERT 3[0m in 8.86s]
[0m12:25:27.619299 [debug] [Thread-1 (]: Finished running node seed.databricks_poc.product_category
[0m12:25:27.621223 [debug] [MainThread]: On master: ROLLBACK
[0m12:25:27.621776 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:25:28.359013 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:25:28.359463 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:25:28.359800 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:25:28.360151 [debug] [MainThread]: On master: ROLLBACK
[0m12:25:28.360484 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:25:28.360799 [debug] [MainThread]: On master: Close
[0m12:25:28.622654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:25:28.623086 [debug] [MainThread]: Connection 'seed.databricks_poc.product_category' was properly closed.
[0m12:25:28.623733 [info ] [MainThread]: 
[0m12:25:28.624234 [info ] [MainThread]: Finished running 2 seeds in 0 hours 0 minutes and 38.49 seconds (38.49s).
[0m12:25:28.625545 [debug] [MainThread]: Command end result
[0m12:25:28.635766 [info ] [MainThread]: 
[0m12:25:28.636412 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:25:28.636951 [info ] [MainThread]: 
[0m12:25:28.637444 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:25:28.639251 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 45.176826, "process_user_time": 13.695042, "process_kernel_time": 1.41186, "process_mem_max_rss": "228244", "process_in_blocks": "211584", "process_out_blocks": "3360"}
[0m12:25:28.639919 [debug] [MainThread]: Command `dbt seed` succeeded at 12:25:28.639775 after 45.18 seconds
[0m12:25:28.640321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1775e0f2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f177744fd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f173b784220>]}
[0m12:25:28.640759 [debug] [MainThread]: Flushing usage events
[0m12:28:58.438591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd74b332b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd72b1f670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd72b1f610>]}


============================== 12:28:58.441088 | 6a546e81-da04-41cf-ad54-a0df4b2f1117 ==============================
[0m12:28:58.441088 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:28:58.442045 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:29:01.423387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd72b1f760>]}
[0m12:29:01.793344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd72abe020>]}
[0m12:29:01.796141 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:29:01.823604 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:29:01.959557 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:29:01.961673 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:29:01.986905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47ea1e40>]}
[0m12:29:02.033581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47e62080>]}
[0m12:29:02.036115 [info ] [MainThread]: Found 2 seeds, 1 model, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m12:29:02.038297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47e62050>]}
[0m12:29:02.046669 [info ] [MainThread]: 
[0m12:29:02.050677 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:29:02.055136 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:29:02.056267 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:29:02.058964 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:29:02.062699 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:29:03.258842 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m12:29:03.270971 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:29:03.686954 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_silver)
[0m12:29:03.688912 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_silver"
"
[0m12:29:03.707966 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:29:03.708436 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_silver"
[0m12:29:03.708808 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_silver"} */
create schema if not exists `hive_metastore`.`poc_silver`
  
[0m12:29:03.709194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:29:05.033889 [debug] [ThreadPool]: SQL status: OK in 1.3200000524520874 seconds
[0m12:29:05.037015 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:29:05.039000 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: ROLLBACK
[0m12:29:05.040791 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:29:05.042508 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: Close
[0m12:29:05.307416 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_silver, now list_hive_metastore_poc_bronze)
[0m12:29:05.322899 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:29:05.323372 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m12:29:05.323708 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:29:06.481075 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m12:29:06.507475 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:29:06.509487 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:29:06.510653 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m12:29:07.001359 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m12:29:07.030186 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:29:07.030755 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m12:29:07.492995 [debug] [ThreadPool]: SQL status: OK in 0.46000000834465027 seconds
[0m12:29:07.500770 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:29:07.501228 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m12:29:08.079882 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m12:29:08.093375 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m12:29:08.095622 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:29:08.097695 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m12:29:08.443881 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now list_hive_metastore_poc_silver)
[0m12:29:08.447305 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:29:08.447734 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m12:29:08.448067 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:29:09.531643 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m12:29:09.534772 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m12:29:09.796839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47d42e30>]}
[0m12:29:09.797412 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:29:09.797789 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:29:09.798447 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:29:09.798895 [info ] [MainThread]: 
[0m12:29:09.806149 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_fact
[0m12:29:09.808881 [info ] [Thread-1 (]: 1 of 1 START sql view model poc_silver.product_fact ............................ [RUN]
[0m12:29:09.813516 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now model.databricks_poc.product_fact)
[0m12:29:09.814388 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_fact
[0m12:29:09.843515 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_fact"
[0m12:29:09.845506 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (compile): 12:29:09.815322 => 12:29:09.844602
[0m12:29:09.846052 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_fact
[0m12:29:09.923840 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_fact"
[0m12:29:09.924553 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:29:09.924895 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_fact"
[0m12:29:09.925400 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_fact"} */
create or replace view `hive_metastore`.`poc_silver`.`product_fact`
  
  
  as
    SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id

[0m12:29:09.927264 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:29:11.671695 [debug] [Thread-1 (]: SQL status: OK in 1.7400000095367432 seconds
[0m12:29:11.681901 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (execute): 12:29:09.846317 => 12:29:11.681672
[0m12:29:11.682474 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: ROLLBACK
[0m12:29:11.682966 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:29:11.683366 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: Close
[0m12:29:11.951830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a546e81-da04-41cf-ad54-a0df4b2f1117', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47d6cd30>]}
[0m12:29:11.952570 [info ] [Thread-1 (]: 1 of 1 OK created sql view model poc_silver.product_fact ....................... [[32mOK[0m in 2.14s]
[0m12:29:11.953368 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_fact
[0m12:29:11.955432 [debug] [MainThread]: On master: ROLLBACK
[0m12:29:11.955804 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:29:12.689326 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:29:12.689771 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:29:12.690112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:29:12.690446 [debug] [MainThread]: On master: ROLLBACK
[0m12:29:12.690777 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:29:12.691105 [debug] [MainThread]: On master: Close
[0m12:29:12.977225 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:29:12.977646 [debug] [MainThread]: Connection 'model.databricks_poc.product_fact' was properly closed.
[0m12:29:12.978241 [info ] [MainThread]: 
[0m12:29:12.978677 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 10.93 seconds (10.93s).
[0m12:29:12.979696 [debug] [MainThread]: Command end result
[0m12:29:12.999534 [info ] [MainThread]: 
[0m12:29:13.000164 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:29:13.000556 [info ] [MainThread]: 
[0m12:29:13.000952 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m12:29:13.001884 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.702074, "process_user_time": 10.92085, "process_kernel_time": 0.987364, "process_mem_max_rss": "219552", "process_in_blocks": "208", "process_out_blocks": "2248"}
[0m12:29:13.002503 [debug] [MainThread]: Command `dbt run` succeeded at 12:29:13.002370 after 14.70 seconds
[0m12:29:13.003628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd74b332b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd4c1d8be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcd47d6c3a0>]}
[0m12:29:13.005708 [debug] [MainThread]: Flushing usage events
[0m12:33:07.581675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42d78df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42b767610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42b7675b0>]}


============================== 12:33:07.584053 | c3d421b9-2c05-4eff-b793-dad40ffa24a1 ==============================
[0m12:33:07.584053 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:33:07.584573 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:33:09.010035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42b767700>]}
[0m12:33:09.194724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42b706110>]}
[0m12:33:09.195343 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:33:09.205868 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:33:09.245367 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m12:33:09.246131 [debug] [MainThread]: Partial parsing: added file: databricks_poc://models/silver/product_delta_tb.sql
[0m12:33:09.447161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb404a680d0>]}
[0m12:33:09.468716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb404ad7940>]}
[0m12:33:09.469226 [info ] [MainThread]: Found 2 seeds, 2 models, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m12:33:09.469640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb404ad7a00>]}
[0m12:33:09.471428 [info ] [MainThread]: 
[0m12:33:09.472551 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:33:09.473965 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:33:09.475592 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:33:09.477533 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:33:09.479222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:33:10.633645 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m12:33:10.636355 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:33:10.909903 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_silver)
[0m12:33:10.910707 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_silver"
"
[0m12:33:10.919682 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:10.920166 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_silver"
[0m12:33:10.920713 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_silver"} */
create schema if not exists `hive_metastore`.`poc_silver`
  
[0m12:33:10.921128 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:33:12.263530 [debug] [ThreadPool]: SQL status: OK in 1.340000033378601 seconds
[0m12:33:12.264520 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:33:12.264897 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: ROLLBACK
[0m12:33:12.265238 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:33:12.265562 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: Close
[0m12:33:12.527295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_silver, now list_hive_metastore_poc_silver)
[0m12:33:12.531927 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:33:12.532473 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m12:33:12.532863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:33:13.598624 [debug] [ThreadPool]: SQL status: OK in 1.0700000524520874 seconds
[0m12:33:13.606334 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:13.606938 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:33:13.607934 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */

      select current_catalog()
  
[0m12:33:14.047607 [debug] [ThreadPool]: SQL status: OK in 0.4399999976158142 seconds
[0m12:33:14.073637 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:33:14.076326 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */
show views in `hive_metastore`.`poc_silver`
  
[0m12:33:14.498429 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m12:33:14.520719 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:33:14.521189 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */
show table extended in `hive_metastore`.`poc_silver` like '*'
  
[0m12:33:14.927956 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m12:33:14.930779 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: ROLLBACK
[0m12:33:14.931193 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:33:14.931543 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m12:33:15.190381 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now list_hive_metastore_poc_bronze)
[0m12:33:15.193943 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:33:15.194413 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m12:33:15.194752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:33:16.245876 [debug] [ThreadPool]: SQL status: OK in 1.0499999523162842 seconds
[0m12:33:16.250803 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:16.251191 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:33:16.251613 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m12:33:16.576980 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m12:33:16.580978 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:33:16.581517 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m12:33:17.010743 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m12:33:17.015301 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:33:17.015719 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m12:33:17.522790 [debug] [ThreadPool]: SQL status: OK in 0.5099999904632568 seconds
[0m12:33:17.533009 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m12:33:17.535227 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:33:17.535821 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m12:33:17.798786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb404d09180>]}
[0m12:33:17.801392 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:17.803446 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:33:17.805480 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:33:17.805939 [info ] [MainThread]: 
[0m12:33:17.814440 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_delta_tb
[0m12:33:17.817319 [info ] [Thread-1 (]: 1 of 2 START sql table model poc_silver.product_delta_tb ....................... [RUN]
[0m12:33:17.819327 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now model.databricks_poc.product_delta_tb)
[0m12:33:17.819746 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_delta_tb
[0m12:33:17.828429 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_delta_tb"
[0m12:33:17.829137 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_delta_tb (compile): 12:33:17.820024 => 12:33:17.828896
[0m12:33:17.829542 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_delta_tb
[0m12:33:17.938268 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_delta_tb"
[0m12:33:17.939018 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:17.939379 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_delta_tb"
[0m12:33:17.940068 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_delta_tb"} */

  
    
        create or replace table `hive_metastore`.`poc_silver`.`product_delta_tb`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id
  
[0m12:33:17.940654 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:33:26.532248 [debug] [Thread-1 (]: SQL status: OK in 8.59000015258789 seconds
[0m12:33:26.821624 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_delta_tb (execute): 12:33:17.829800 => 12:33:26.820706
[0m12:33:26.822109 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: ROLLBACK
[0m12:33:26.822565 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:33:26.823209 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: Close
[0m12:33:27.081057 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb40414a920>]}
[0m12:33:27.081788 [info ] [Thread-1 (]: 1 of 2 OK created sql table model poc_silver.product_delta_tb .................. [[32mOK[0m in 9.26s]
[0m12:33:27.082629 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_delta_tb
[0m12:33:27.083564 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_fact
[0m12:33:27.084244 [info ] [Thread-1 (]: 2 of 2 START sql view model poc_silver.product_fact ............................ [RUN]
[0m12:33:27.085006 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.databricks_poc.product_delta_tb, now model.databricks_poc.product_fact)
[0m12:33:27.085432 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_fact
[0m12:33:27.092368 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_fact"
[0m12:33:27.096531 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (compile): 12:33:27.085691 => 12:33:27.094964
[0m12:33:27.098930 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_fact
[0m12:33:27.125725 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_fact"
[0m12:33:27.126452 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:27.126796 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_fact"
[0m12:33:27.127234 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_fact"} */
create or replace view `hive_metastore`.`poc_silver`.`product_fact`
  
  
  as
    SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id

[0m12:33:27.127643 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:33:29.227409 [debug] [Thread-1 (]: SQL status: OK in 2.0999999046325684 seconds
[0m12:33:29.232119 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_fact (execute): 12:33:27.100294 => 12:33:29.231890
[0m12:33:29.232575 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: ROLLBACK
[0m12:33:29.233002 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:33:29.233399 [debug] [Thread-1 (]: On model.databricks_poc.product_fact: Close
[0m12:33:29.486885 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3d421b9-2c05-4eff-b793-dad40ffa24a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3f7148160>]}
[0m12:33:29.487695 [info ] [Thread-1 (]: 2 of 2 OK created sql view model poc_silver.product_fact ....................... [[32mOK[0m in 2.40s]
[0m12:33:29.488425 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_fact
[0m12:33:29.490498 [debug] [MainThread]: On master: ROLLBACK
[0m12:33:29.490926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:33:30.259547 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:33:30.260172 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:33:30.260566 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:33:30.260920 [debug] [MainThread]: On master: ROLLBACK
[0m12:33:30.261287 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:33:30.261674 [debug] [MainThread]: On master: Close
[0m12:33:30.535339 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:33:30.535757 [debug] [MainThread]: Connection 'model.databricks_poc.product_fact' was properly closed.
[0m12:33:30.536369 [info ] [MainThread]: 
[0m12:33:30.536789 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 21.06 seconds (21.06s).
[0m12:33:30.537980 [debug] [MainThread]: Command end result
[0m12:33:30.559025 [info ] [MainThread]: 
[0m12:33:30.559552 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:33:30.559949 [info ] [MainThread]: 
[0m12:33:30.560391 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:33:30.561574 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 23.044254, "process_user_time": 5.596573, "process_kernel_time": 0.995831, "process_mem_max_rss": "226476", "process_out_blocks": "3392", "process_in_blocks": "0"}
[0m12:33:30.562146 [debug] [MainThread]: Command `dbt run` succeeded at 12:33:30.562010 after 23.04 seconds
[0m12:33:30.562531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42d78df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb42b706110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb404ae9ff0>]}
[0m12:33:30.562951 [debug] [MainThread]: Flushing usage events
[0m12:39:11.919701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9676c09f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9674bbb610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9674bbb5b0>]}


============================== 12:39:11.928836 | 7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0 ==============================
[0m12:39:11.928836 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:39:11.931535 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m12:39:15.500115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9674bbb700>]}
[0m12:39:16.084498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9674b5a110>]}
[0m12:39:16.087263 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:39:16.134566 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:39:16.256655 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 1 files added, 0 files changed.
[0m12:39:16.257234 [debug] [MainThread]: Partial parsing: added file: databricks_poc://models/silver/product_vw.sql
[0m12:39:16.257605 [debug] [MainThread]: Partial parsing: deleted file: databricks_poc://models/silver/product_delta_tb.sql
[0m12:39:16.257944 [debug] [MainThread]: Partial parsing: deleted file: databricks_poc://models/silver/product_fact.sql
[0m12:39:16.705190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9649f840d0>]}
[0m12:39:16.744787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f964a006290>]}
[0m12:39:16.747340 [info ] [MainThread]: Found 2 seeds, 1 model, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m12:39:16.749485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f964a0063e0>]}
[0m12:39:16.754729 [info ] [MainThread]: 
[0m12:39:16.758736 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:39:16.765597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:39:16.766812 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:39:16.768666 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:39:16.770308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:39:17.852625 [debug] [ThreadPool]: SQL status: OK in 1.0800000429153442 seconds
[0m12:39:17.863843 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:39:18.149800 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_silver)
[0m12:39:18.151658 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_silver"
"
[0m12:39:18.188117 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:18.190129 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_silver"
[0m12:39:18.192372 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_silver"} */
create schema if not exists `hive_metastore`.`poc_silver`
  
[0m12:39:18.193190 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:19.298789 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m12:39:19.299793 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:39:19.300158 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: ROLLBACK
[0m12:39:19.300508 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:39:19.300855 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: Close
[0m12:39:19.562996 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_silver, now list_hive_metastore_poc_bronze)
[0m12:39:19.585439 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:39:19.586539 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m12:39:19.588488 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:20.734267 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m12:39:20.778527 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:20.780677 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:39:20.782703 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m12:39:21.147268 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m12:39:21.198296 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:39:21.200814 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m12:39:21.609027 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m12:39:21.643418 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:39:21.645878 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m12:39:22.184373 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m12:39:22.199645 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m12:39:22.201785 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:39:22.203688 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m12:39:22.501746 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now list_hive_metastore_poc_silver)
[0m12:39:22.519318 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:39:22.520615 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m12:39:22.521028 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:39:23.617479 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m12:39:23.636480 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m12:39:23.928617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96486383a0>]}
[0m12:39:23.931201 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:23.933534 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:39:23.937716 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:39:23.939673 [info ] [MainThread]: 
[0m12:39:23.951214 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_vw
[0m12:39:23.954394 [info ] [Thread-1 (]: 1 of 1 START sql view model poc_silver.product_vw .............................. [RUN]
[0m12:39:23.958518 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now model.databricks_poc.product_vw)
[0m12:39:23.962550 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_vw
[0m12:39:24.011619 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_vw"
[0m12:39:24.018088 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_vw (compile): 12:39:23.964168 => 12:39:24.015680
[0m12:39:24.020378 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_vw
[0m12:39:24.160924 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_vw"
[0m12:39:24.164610 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:24.165710 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_vw"
[0m12:39:24.166738 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_vw"} */
create or replace view `hive_metastore`.`poc_silver`.`product_vw`
  
  
  as
    SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id

[0m12:39:24.169049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:39:25.961224 [debug] [Thread-1 (]: SQL status: OK in 1.7899999618530273 seconds
[0m12:39:26.014204 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_vw (execute): 12:39:24.022311 => 12:39:26.012386
[0m12:39:26.016779 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: ROLLBACK
[0m12:39:26.017959 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:39:26.020101 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: Close
[0m12:39:26.283251 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7cc4f5a3-bd01-49d5-8f51-7818bf38f9f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f96486028f0>]}
[0m12:39:26.289887 [info ] [Thread-1 (]: 1 of 1 OK created sql view model poc_silver.product_vw ......................... [[32mOK[0m in 2.32s]
[0m12:39:26.295936 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_vw
[0m12:39:26.303840 [debug] [MainThread]: On master: ROLLBACK
[0m12:39:26.305898 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:39:27.140757 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:39:27.141207 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:39:27.141593 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:39:27.142032 [debug] [MainThread]: On master: ROLLBACK
[0m12:39:27.143458 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:39:27.145323 [debug] [MainThread]: On master: Close
[0m12:39:27.414226 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:39:27.416089 [debug] [MainThread]: Connection 'model.databricks_poc.product_vw' was properly closed.
[0m12:39:27.418761 [info ] [MainThread]: 
[0m12:39:27.420953 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 10.66 seconds (10.66s).
[0m12:39:27.425111 [debug] [MainThread]: Command end result
[0m12:39:27.452245 [info ] [MainThread]: 
[0m12:39:27.454598 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:39:27.455881 [info ] [MainThread]: 
[0m12:39:27.457259 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m12:39:27.461109 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.723766, "process_user_time": 15.176996, "process_kernel_time": 1.279746, "process_mem_max_rss": "221316", "process_out_blocks": "3352", "process_in_blocks": "0"}
[0m12:39:27.464245 [debug] [MainThread]: Command `dbt run` succeeded at 12:39:27.463589 after 15.73 seconds
[0m12:39:27.466340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9676c09f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9678237d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9649ff2890>]}
[0m12:39:27.468519 [debug] [MainThread]: Flushing usage events
[0m12:42:44.688074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24deb2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac22dc3640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac22dc35e0>]}


============================== 12:42:44.690495 | 9b4b6df2-5799-4606-bfb9-ffa31734793e ==============================
[0m12:42:44.690495 [info ] [MainThread]: Running with dbt=1.7.1
[0m12:42:44.691031 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/bala/apps/pocs/dbt/databricks_poc/logs', 'fail_fast': 'False', 'profiles_dir': '/home/bala/apps/pocs/dbt/databricks_poc', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:42:46.354885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac22dc3730>]}
[0m12:42:46.602700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac22d647f0>]}
[0m12:42:46.603485 [info ] [MainThread]: Registered adapter: databricks=1.7.0
[0m12:42:46.614054 [debug] [MainThread]: checksum: ad428e6dc3b353dcb23d0c4ebea8b20b180e94b873271ac2a054a8e446dac0e2, vars: {}, profile: , target: , version: 1.7.1
[0m12:42:46.652648 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m12:42:46.653470 [debug] [MainThread]: Partial parsing: added file: databricks_poc://models/silver/product_delta_tb.sql
[0m12:42:46.937345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabf7fe00d0>]}
[0m12:42:46.959041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabfc143a90>]}
[0m12:42:46.959576 [info ] [MainThread]: Found 2 seeds, 2 models, 2 sources, 0 exposures, 0 metrics, 525 macros, 0 groups, 0 semantic models
[0m12:42:46.959983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabfc143b50>]}
[0m12:42:46.961745 [info ] [MainThread]: 
[0m12:42:46.962614 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:42:46.967251 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m12:42:46.970136 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m12:42:46.972452 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m12:42:46.975075 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:42:48.021548 [debug] [ThreadPool]: SQL status: OK in 1.0499999523162842 seconds
[0m12:42:48.024360 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m12:42:48.315872 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_poc_silver)
[0m12:42:48.319930 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "poc_silver"
"
[0m12:42:48.356675 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:42:48.358646 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_poc_silver"
[0m12:42:48.360590 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "create_hive_metastore_poc_silver"} */
create schema if not exists `hive_metastore`.`poc_silver`
  
[0m12:42:48.362374 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:42:49.477296 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m12:42:49.478345 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:42:49.478722 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: ROLLBACK
[0m12:42:49.479079 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:42:49.479412 [debug] [ThreadPool]: On create_hive_metastore_poc_silver: Close
[0m12:42:49.752693 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_poc_silver, now list_hive_metastore_poc_silver)
[0m12:42:49.769051 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:42:49.769540 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: GetTables(database=hive_metastore, schema=poc_silver, identifier=None)
[0m12:42:49.769928 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:42:50.916616 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m12:42:50.924589 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:42:50.925034 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:42:50.925396 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */

      select current_catalog()
  
[0m12:42:51.355152 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m12:42:51.381789 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:42:51.383981 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */
show views in `hive_metastore`.`poc_silver`
  
[0m12:42:51.812618 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m12:42:51.821837 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_silver"
[0m12:42:51.822889 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_silver"} */
show table extended in `hive_metastore`.`poc_silver` like '*'
  
[0m12:42:52.234929 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m12:42:52.238268 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: ROLLBACK
[0m12:42:52.239022 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:42:52.239566 [debug] [ThreadPool]: On list_hive_metastore_poc_silver: Close
[0m12:42:52.499869 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_silver, now list_hive_metastore_poc_bronze)
[0m12:42:52.502548 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:42:52.502982 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: GetTables(database=hive_metastore, schema=poc_bronze, identifier=None)
[0m12:42:52.503373 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:42:53.604353 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m12:42:53.610014 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:42:53.610431 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:42:53.610810 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */

      select current_catalog()
  
[0m12:42:53.943048 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m12:42:53.947026 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:42:53.947433 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show views in `hive_metastore`.`poc_bronze`
  
[0m12:42:54.338890 [debug] [ThreadPool]: SQL status: OK in 0.38999998569488525 seconds
[0m12:42:54.343373 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_poc_bronze"
[0m12:42:54.343885 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "connection_name": "list_hive_metastore_poc_bronze"} */
show table extended in `hive_metastore`.`poc_bronze` like '*'
  
[0m12:42:54.829462 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m12:42:54.832295 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: ROLLBACK
[0m12:42:54.832715 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:42:54.833102 [debug] [ThreadPool]: On list_hive_metastore_poc_bronze: Close
[0m12:42:55.085215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabfc316da0>]}
[0m12:42:55.085736 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:42:55.086112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:42:55.086765 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:42:55.087250 [info ] [MainThread]: 
[0m12:42:55.090290 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_delta_tb
[0m12:42:55.090852 [info ] [Thread-1 (]: 1 of 2 START sql table model poc_silver.product_delta_tb ....................... [RUN]
[0m12:42:55.092055 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_poc_bronze, now model.databricks_poc.product_delta_tb)
[0m12:42:55.092485 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_delta_tb
[0m12:42:55.100438 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_delta_tb"
[0m12:42:55.101114 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_delta_tb (compile): 12:42:55.092753 => 12:42:55.100869
[0m12:42:55.101709 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_delta_tb
[0m12:42:55.196577 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_delta_tb"
[0m12:42:55.197289 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:42:55.197657 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_delta_tb"
[0m12:42:55.198091 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_delta_tb"} */

  
    
        create or replace table `hive_metastore`.`poc_silver`.`product_delta_tb`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id
  
[0m12:42:55.198517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:43:00.471366 [debug] [Thread-1 (]: SQL status: OK in 5.269999980926514 seconds
[0m12:43:00.505751 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_delta_tb (execute): 12:42:55.101976 => 12:43:00.505374
[0m12:43:00.506511 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: ROLLBACK
[0m12:43:00.507171 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:43:00.507623 [debug] [Thread-1 (]: On model.databricks_poc.product_delta_tb: Close
[0m12:43:00.774677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabf68d2980>]}
[0m12:43:00.775416 [info ] [Thread-1 (]: 1 of 2 OK created sql table model poc_silver.product_delta_tb .................. [[32mOK[0m in 5.68s]
[0m12:43:00.776444 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_delta_tb
[0m12:43:00.777337 [debug] [Thread-1 (]: Began running node model.databricks_poc.product_vw
[0m12:43:00.778593 [info ] [Thread-1 (]: 2 of 2 START sql view model poc_silver.product_vw .............................. [RUN]
[0m12:43:00.779775 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.databricks_poc.product_delta_tb, now model.databricks_poc.product_vw)
[0m12:43:00.780296 [debug] [Thread-1 (]: Began compiling node model.databricks_poc.product_vw
[0m12:43:00.784120 [debug] [Thread-1 (]: Writing injected SQL for node "model.databricks_poc.product_vw"
[0m12:43:00.784850 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_vw (compile): 12:43:00.780647 => 12:43:00.784608
[0m12:43:00.785274 [debug] [Thread-1 (]: Began executing node model.databricks_poc.product_vw
[0m12:43:00.811602 [debug] [Thread-1 (]: Writing runtime sql for node "model.databricks_poc.product_vw"
[0m12:43:00.812401 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:43:00.812928 [debug] [Thread-1 (]: Using databricks connection "model.databricks_poc.product_vw"
[0m12:43:00.813351 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: /* {"app": "dbt", "dbt_version": "1.7.1", "dbt_databricks_version": "1.7.0", "databricks_sql_connector_version": "2.9.3", "profile_name": "databricks_poc", "target_name": "dev", "node_id": "model.databricks_poc.product_vw"} */
create or replace view `hive_metastore`.`poc_silver`.`product_vw`
  
  
  as
    SELECT 
    P.product_id,
    P.name as product_name,
    P.unit_price,
    C.name as category_name
FROM `hive_metastore`.`poc_bronze`.`product` P
LEFT JOIN `hive_metastore`.`poc_bronze`.`product_category` C
    ON C.category_id = P.category_id

[0m12:43:00.813777 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:43:02.714533 [debug] [Thread-1 (]: SQL status: OK in 1.899999976158142 seconds
[0m12:43:02.719163 [debug] [Thread-1 (]: Timing info for model.databricks_poc.product_vw (execute): 12:43:00.785601 => 12:43:02.718924
[0m12:43:02.719616 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: ROLLBACK
[0m12:43:02.720033 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:43:02.720447 [debug] [Thread-1 (]: On model.databricks_poc.product_vw: Close
[0m12:43:02.976449 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b4b6df2-5799-4606-bfb9-ffa31734793e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabf67581c0>]}
[0m12:43:02.977173 [info ] [Thread-1 (]: 2 of 2 OK created sql view model poc_silver.product_vw ......................... [[32mOK[0m in 2.20s]
[0m12:43:02.977862 [debug] [Thread-1 (]: Finished running node model.databricks_poc.product_vw
[0m12:43:02.979667 [debug] [MainThread]: On master: ROLLBACK
[0m12:43:02.980174 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:43:03.686443 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:43:03.686899 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:43:03.687246 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:43:03.687777 [debug] [MainThread]: On master: ROLLBACK
[0m12:43:03.688192 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:43:03.688698 [debug] [MainThread]: On master: Close
[0m12:43:03.997644 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:43:03.998092 [debug] [MainThread]: Connection 'model.databricks_poc.product_vw' was properly closed.
[0m12:43:03.998728 [info ] [MainThread]: 
[0m12:43:03.999173 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 17.04 seconds (17.04s).
[0m12:43:04.000322 [debug] [MainThread]: Command end result
[0m12:43:04.010843 [info ] [MainThread]: 
[0m12:43:04.011459 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:43:04.011925 [info ] [MainThread]: 
[0m12:43:04.012431 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m12:43:04.013413 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.379236, "process_user_time": 7.370927, "process_kernel_time": 0.961425, "process_mem_max_rss": "226996", "process_out_blocks": "3384", "process_in_blocks": "0"}
[0m12:43:04.014017 [debug] [MainThread]: Command `dbt run` succeeded at 12:43:04.013878 after 19.38 seconds
[0m12:43:04.014443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac24deb2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac235a84c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabfc1542e0>]}
[0m12:43:04.014855 [debug] [MainThread]: Flushing usage events
